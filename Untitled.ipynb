{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff94563-fe91-4d4e-9b56-b8965a44bf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-03 17:39:56.538231\n",
      "2024-07-03-17-39-56\n",
      "errors_2024-07-03-17-39-56.txt\n",
      "(154962, 18)\n",
      "(81446, 18)\n",
      "(236408, 17)\n",
      "(236408,)\n",
      "(236408, 17)\n",
      "(236408,)\n",
      "******************************\n",
      "Decison Tree Completed :) \n",
      "******************************\n",
      "Linear Regression Completed :) \n",
      "******************************\n",
      "LogisticRegression Completed :) \n",
      "******************************\n",
      "KNN Completed :) \n",
      "******************************\n",
      "Random forest Completed :) \n",
      "******************************\n",
      "MLP Completed :) \n",
      "******************************\n",
      "Bagging Completed :) \n",
      "******************************\n",
      "J48 Completed :) \n",
      "******************************\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "ANN Completed :) \n",
      "******************************\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "DNN Completed :) \n",
      "******************************\n",
      "GradientBoostingClassifier Completed :) \n",
      "******************************\n",
      "XGBClassifier Completed :) \n",
      "******************************\n",
      "Gaussian_Naive_Bayes Completed :) \n",
      "******************************\n",
      "Adaptive Gradient Boosting Completed :) \n",
      "******************************\n",
      "QDA Completed :) \n",
      "******************************\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "SNN Completed :)  \n",
      "******************************\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "RBM Completed :)  \n",
      "******************************\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step\n",
      "lstm Completed :)  \n",
      "******************************\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "reconstruction neural networks, Completed :)  \n",
      "******************************\n",
      "Epoch 1/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1.2519 - source_classifier_accuracy: 0.5646 - val_loss: 0.8532 - val_source_classifier_accuracy: 0.6855\n",
      "Epoch 2/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.8374 - source_classifier_accuracy: 0.6985 - val_loss: 0.7206 - val_source_classifier_accuracy: 0.7608\n",
      "Epoch 3/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.7367 - source_classifier_accuracy: 0.7460 - val_loss: 0.6635 - val_source_classifier_accuracy: 0.7871\n",
      "Epoch 4/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.6799 - source_classifier_accuracy: 0.7675 - val_loss: 0.6275 - val_source_classifier_accuracy: 0.7880\n",
      "Epoch 5/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.6491 - source_classifier_accuracy: 0.7779 - val_loss: 0.6025 - val_source_classifier_accuracy: 0.7985\n",
      "Epoch 6/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6298 - source_classifier_accuracy: 0.7845 - val_loss: 0.5872 - val_source_classifier_accuracy: 0.8031\n",
      "Epoch 7/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6169 - source_classifier_accuracy: 0.7873 - val_loss: 0.5725 - val_source_classifier_accuracy: 0.8064\n",
      "Epoch 8/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6015 - source_classifier_accuracy: 0.7931 - val_loss: 0.5621 - val_source_classifier_accuracy: 0.8111\n",
      "Epoch 9/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.5959 - source_classifier_accuracy: 0.7942 - val_loss: 0.5530 - val_source_classifier_accuracy: 0.8134\n",
      "Epoch 10/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.5823 - source_classifier_accuracy: 0.7990 - val_loss: 0.5462 - val_source_classifier_accuracy: 0.8159\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 1.2418 - source_classifier_accuracy: 0.5669 - val_loss: 0.8441 - val_source_classifier_accuracy: 0.6988\n",
      "Epoch 2/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.8329 - source_classifier_accuracy: 0.7039 - val_loss: 0.7145 - val_source_classifier_accuracy: 0.7594\n",
      "Epoch 3/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.7345 - source_classifier_accuracy: 0.7480 - val_loss: 0.6525 - val_source_classifier_accuracy: 0.7764\n",
      "Epoch 4/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6799 - source_classifier_accuracy: 0.7668 - val_loss: 0.6168 - val_source_classifier_accuracy: 0.7877\n",
      "Epoch 5/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.6497 - source_classifier_accuracy: 0.7760 - val_loss: 0.5940 - val_source_classifier_accuracy: 0.8005\n",
      "Epoch 6/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6267 - source_classifier_accuracy: 0.7832 - val_loss: 0.5783 - val_source_classifier_accuracy: 0.8006\n",
      "Epoch 7/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6075 - source_classifier_accuracy: 0.7887 - val_loss: 0.5615 - val_source_classifier_accuracy: 0.8083\n",
      "Epoch 8/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.5975 - source_classifier_accuracy: 0.7935 - val_loss: 0.5535 - val_source_classifier_accuracy: 0.8124\n",
      "Epoch 9/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.5862 - source_classifier_accuracy: 0.7974 - val_loss: 0.5437 - val_source_classifier_accuracy: 0.8176\n",
      "Epoch 10/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.5785 - source_classifier_accuracy: 0.7995 - val_loss: 0.5327 - val_source_classifier_accuracy: 0.8196\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1.2516 - source_classifier_accuracy: 0.5613 - val_loss: 0.8426 - val_source_classifier_accuracy: 0.7016\n",
      "Epoch 2/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.8312 - source_classifier_accuracy: 0.7036 - val_loss: 0.7094 - val_source_classifier_accuracy: 0.7594\n",
      "Epoch 3/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.7280 - source_classifier_accuracy: 0.7497 - val_loss: 0.6532 - val_source_classifier_accuracy: 0.7817\n",
      "Epoch 4/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6787 - source_classifier_accuracy: 0.7665 - val_loss: 0.6239 - val_source_classifier_accuracy: 0.7907\n",
      "Epoch 5/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6528 - source_classifier_accuracy: 0.7745 - val_loss: 0.5999 - val_source_classifier_accuracy: 0.7992\n",
      "Epoch 6/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6297 - source_classifier_accuracy: 0.7809 - val_loss: 0.5827 - val_source_classifier_accuracy: 0.8041\n",
      "Epoch 7/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6205 - source_classifier_accuracy: 0.7841 - val_loss: 0.5726 - val_source_classifier_accuracy: 0.8037\n",
      "Epoch 8/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6034 - source_classifier_accuracy: 0.7908 - val_loss: 0.5620 - val_source_classifier_accuracy: 0.8138\n",
      "Epoch 9/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.5944 - source_classifier_accuracy: 0.7931 - val_loss: 0.5522 - val_source_classifier_accuracy: 0.8130\n",
      "Epoch 10/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.5893 - source_classifier_accuracy: 0.7950 - val_loss: 0.5450 - val_source_classifier_accuracy: 0.8185\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 1.2510 - source_classifier_accuracy: 0.5663 - val_loss: 0.8485 - val_source_classifier_accuracy: 0.6882\n",
      "Epoch 2/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.8458 - source_classifier_accuracy: 0.6963 - val_loss: 0.7145 - val_source_classifier_accuracy: 0.7532\n",
      "Epoch 3/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.7396 - source_classifier_accuracy: 0.7461 - val_loss: 0.6525 - val_source_classifier_accuracy: 0.7706\n",
      "Epoch 4/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6838 - source_classifier_accuracy: 0.7633 - val_loss: 0.6153 - val_source_classifier_accuracy: 0.7899\n",
      "Epoch 5/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6579 - source_classifier_accuracy: 0.7731 - val_loss: 0.5921 - val_source_classifier_accuracy: 0.7992\n",
      "Epoch 6/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6280 - source_classifier_accuracy: 0.7818 - val_loss: 0.5743 - val_source_classifier_accuracy: 0.8049\n",
      "Epoch 7/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.6165 - source_classifier_accuracy: 0.7876 - val_loss: 0.5589 - val_source_classifier_accuracy: 0.8105\n",
      "Epoch 8/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.6024 - source_classifier_accuracy: 0.7902 - val_loss: 0.5495 - val_source_classifier_accuracy: 0.8144\n",
      "Epoch 9/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.5925 - source_classifier_accuracy: 0.7937 - val_loss: 0.5395 - val_source_classifier_accuracy: 0.8187\n",
      "Epoch 10/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.5817 - source_classifier_accuracy: 0.7980 - val_loss: 0.5325 - val_source_classifier_accuracy: 0.8212\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 1.2290 - source_classifier_accuracy: 0.5745 - val_loss: 0.8315 - val_source_classifier_accuracy: 0.6885\n",
      "Epoch 2/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.8238 - source_classifier_accuracy: 0.7071 - val_loss: 0.7054 - val_source_classifier_accuracy: 0.7661\n",
      "Epoch 3/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.7295 - source_classifier_accuracy: 0.7489 - val_loss: 0.6484 - val_source_classifier_accuracy: 0.7785\n",
      "Epoch 4/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.6795 - source_classifier_accuracy: 0.7637 - val_loss: 0.6192 - val_source_classifier_accuracy: 0.7943\n",
      "Epoch 5/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.6465 - source_classifier_accuracy: 0.7763 - val_loss: 0.5905 - val_source_classifier_accuracy: 0.8032\n",
      "Epoch 6/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6254 - source_classifier_accuracy: 0.7832 - val_loss: 0.5745 - val_source_classifier_accuracy: 0.8058\n",
      "Epoch 7/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.6104 - source_classifier_accuracy: 0.7888 - val_loss: 0.5611 - val_source_classifier_accuracy: 0.8110\n",
      "Epoch 8/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.5999 - source_classifier_accuracy: 0.7908 - val_loss: 0.5490 - val_source_classifier_accuracy: 0.8152\n",
      "Epoch 9/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.5865 - source_classifier_accuracy: 0.7975 - val_loss: 0.5413 - val_source_classifier_accuracy: 0.8135\n",
      "Epoch 10/10\n",
      "\u001b[1m2956/2956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.5767 - source_classifier_accuracy: 0.7994 - val_loss: 0.5349 - val_source_classifier_accuracy: 0.8178\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "DANN Completed :)  \n",
      "******************************\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -1.70, time = 4.68s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -1.65, time = 7.54s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -1.66, time = 7.35s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -1.66, time = 6.10s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -1.72, time = 5.72s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -1.59, time = 5.51s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.67, time = 6.08s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -1.70, time = 6.06s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -1.64, time = 5.75s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.75, time = 5.53s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -1.66, time = 6.19s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -1.66, time = 5.68s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -1.64, time = 5.99s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -1.76, time = 5.70s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -1.65, time = 5.75s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -1.63, time = 5.61s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -1.62, time = 5.54s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -1.63, time = 5.51s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -1.58, time = 5.55s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -1.56, time = 6.07s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -1.67, time = 4.48s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -1.62, time = 5.88s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -1.61, time = 5.98s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -1.59, time = 5.72s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -1.66, time = 5.75s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -1.56, time = 5.76s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.62, time = 6.00s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -1.62, time = 6.19s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -1.59, time = 5.78s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.62, time = 5.87s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -1.62, time = 7.21s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -1.64, time = 6.24s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -1.60, time = 5.60s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -1.71, time = 5.69s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -1.59, time = 5.58s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -1.57, time = 5.61s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -1.58, time = 5.60s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -1.56, time = 5.63s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -1.56, time = 5.60s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -1.56, time = 5.66s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -1.68, time = 4.30s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -1.64, time = 5.65s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -1.60, time = 5.33s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -1.60, time = 5.16s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -1.65, time = 5.27s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -1.57, time = 5.25s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.62, time = 5.40s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -1.65, time = 5.41s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -1.59, time = 5.35s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.64, time = 5.07s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -1.62, time = 5.11s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -1.63, time = 5.07s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -1.62, time = 5.10s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -1.65, time = 5.47s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -1.62, time = 5.52s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -1.59, time = 5.41s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -1.60, time = 5.38s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -1.57, time = 5.38s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -1.54, time = 5.42s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -1.55, time = 5.47s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -1.68, time = 3.94s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -1.61, time = 5.16s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -1.63, time = 5.48s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -1.59, time = 5.59s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -1.65, time = 5.80s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -1.57, time = 5.75s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.61, time = 5.58s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -1.62, time = 5.30s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -1.57, time = 5.44s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.66, time = 5.57s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -1.62, time = 5.54s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -1.66, time = 5.49s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -1.58, time = 5.36s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -1.65, time = 5.51s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -1.60, time = 6.19s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -1.58, time = 6.00s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -1.59, time = 5.93s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -1.59, time = 6.16s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -1.56, time = 5.54s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -1.56, time = 5.96s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -1.67, time = 4.84s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -1.63, time = 5.81s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -1.65, time = 5.67s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -1.61, time = 5.58s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -1.65, time = 5.59s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -1.57, time = 5.67s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.62, time = 5.74s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -1.65, time = 5.52s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -1.58, time = 5.42s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.63, time = 6.35s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -1.62, time = 5.83s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -1.63, time = 6.19s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -1.59, time = 6.25s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -1.69, time = 6.45s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -1.59, time = 6.26s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -1.58, time = 6.15s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -1.60, time = 6.08s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -1.59, time = 5.93s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -1.56, time = 6.02s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -1.57, time = 5.91s\n",
      "******************************\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "******************************\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "******************************\n",
      "******************************\n",
      "******************************\n",
      "******************************\n",
      "******************************\n",
      "******************************\n",
      "******************************\n",
      "******************************\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step\n"
     ]
    }
   ],
   "source": [
    "n_splits_for_cv = 5 #Dont Change \n",
    "## **Importing Modules and Libraries**\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "start_time_1 = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# Get the current time in the desired format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(start_time_1)\n",
    "print(timestamp)\n",
    "# Generate the filename with the timestamp\n",
    "log_filename = f\"errors_{timestamp}.txt\"\n",
    "print(log_filename)\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=log_filename, level=logging.ERROR, filemode='a')\n",
    "\n",
    "import os\n",
    "# Suppress TensorFlow GPU-related warnings\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from os import path\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten , Activation, SimpleRNN, LSTM, GRU, Dropout, TimeDistributed, Reshape, Input, Lambda, Add\n",
    "from keras import Sequential\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "import sklearn.discriminant_analysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "import skfuzzy as fuzz\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier, AdaBoostClassifier, HistGradientBoostingClassifier, IsolationForest, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import skfuzzy as fuzz\n",
    "from pgmpy.estimators import TreeSearch\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Activation, SimpleRNN, LSTM, GRU, Dropout, TimeDistributed, Reshape, Input, Lambda, Add\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pgmpy.models import BayesianModel\n",
    "from pomegranate import *\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import JunctionTree\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from catboost import CatBoostClassifier\n",
    "import tensorflow as tf\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import ExpectationMaximization\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.datasets import make_classification\n",
    "from pgmpy.models import MarkovModel\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\"\"\"## **Importing Datasets**\"\"\"\n",
    "\n",
    "# dont change this\n",
    "method = \"UNSW_NB_15_TSA_SMOTE_Cosine\"\n",
    "method = method + \"_k_fold_5_Metrics\"\n",
    "\n",
    "train_data = pd.read_csv('UNSW_NB_15_TSA_SMOTE_Cosine.csv')\n",
    "test_data = pd.read_csv('UNSW_NB_15_TSA_test.csv')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# **MULTI-CLASS CLASSIFICATION**\n",
    "## **Data Splitting**\n",
    "X_train = train_data.drop(columns=['label'],axis=1)\n",
    "X_test = test_data.drop(columns=['label'],axis=1)\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "X_train = pd.concat([X_train, X_test], axis=0)\n",
    "y_train = pd.concat([y_train, y_test], axis=0)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "fname = method + \"_output.csv\"\n",
    "outfile = open(fname, 'w')\n",
    "outfile.write(\"algo vs matrices,time to train(sec),time to predict(sec),accuracy_score,precision_score,recall_score,f1_score,fbeta_score,matthews_corrcoef,jaccard_score,cohen_kappa_score,hamming_loss,zero_one_loss,mean_absolute_error,mean_squared_error,mean_squared_error,balanced_accuracy_score,explained_variance_score\\n\")\n",
    "def format_decimal(number):\n",
    "    return f\"{number:.{3}f}\"\n",
    "def result(y_pred,y_test,algo,time_to_predict,time_to_train):\n",
    "    outfile.write(algo+\",\")\n",
    "    outfile.write(str(format_decimal(time_to_train))+\",\")\n",
    "    outfile.write(str(format_decimal(time_to_predict))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.accuracy_score(y_test,y_pred)))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.precision_score(y_test, y_pred, average='weighted')))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.recall_score(y_test, y_pred, average='weighted')))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.f1_score(y_test, y_pred, average='weighted')))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.fbeta_score(y_test, y_pred,average='weighted', beta=0.5)))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.matthews_corrcoef(y_test, y_pred)))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.jaccard_score(y_test, y_pred, average='weighted')))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.cohen_kappa_score(y_test, y_pred)))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.hamming_loss(y_test, y_pred)))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.zero_one_loss(y_test, y_pred)))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.mean_absolute_error(y_test, y_pred)))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.mean_squared_error(y_test, y_pred)))+\",\")\n",
    "    outfile.write(str(format_decimal(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.balanced_accuracy_score(y_test, y_pred)))+\",\")\n",
    "    outfile.write(str(format_decimal(metrics.explained_variance_score(y_test, y_pred)*100))+\"\\n\")\n",
    "\n",
    "#X_train,temp1,y_train,temp2 = train_test_split(X_train,y_train,train_size=0.1,random_state=7)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# Initialize Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits= n_splits_for_cv, random_state=47, shuffle=True)\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "def separator(algo=\"temp\"):\n",
    "    with open(\"errors.txt\", \"a\") as file:\n",
    "        #file.write(datetime.now().strftime(\"%d %b %Y %H:%M\"))\n",
    "        file.write(\"\\n\\n*********\\n\\n\")\n",
    "    outfile.write(algo.strip()+ \" \" + \"erroralgo,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **1.Decision Tree**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test = []\n",
    "    all_y_pred = []\n",
    "    start_cv = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train = 0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize Decision Tree Classifier for this fold\n",
    "        dt_multi = DecisionTreeClassifier(random_state=24)\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Fit the model\n",
    "        start_train = time.time()\n",
    "        dt_multi.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train - start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = dt_multi.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        time_to_predict +=  end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test.extend(y_test_fold)\n",
    "        all_y_pred.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = multilabel_confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize']=8,8\n",
    "        sns.set_style(\"white\")\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix\")\n",
    "        i = str(fold_number)\n",
    "        pname = method + \"_fold_\"+ i + \"_Decision_Tree_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv = time.time()\n",
    "    result(all_y_pred, all_y_test, \"DT\", time_to_train, time_to_predict)\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Decison Tree Completed :) \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Decison Tree\")\n",
    "\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **2.Linear Regression**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_lr = []\n",
    "    all_y_pred_lr = []\n",
    "    start_cv_lr = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train = 0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize Linear Regression model for this fold\n",
    "        lr_multi = LinearRegression()\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Fit the model\n",
    "        start_train = time.time()\n",
    "        lr_multi.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train+=end_train - start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = lr_multi.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        time_to_predict +=  end_predict - start_predict\n",
    "        for i in range(len(y_pred_fold)):\n",
    "            y_pred_fold[i] = int(np.round_(y_pred_fold[i]))\n",
    "        # Append metrics to lists\n",
    "        all_y_test_lr.extend(y_test_fold)\n",
    "        all_y_pred_lr.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize']=8,8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Linear Regression\")\n",
    "        pname = method + \"_fold_\"+ str(fold_number) + \"_Linear_Regression_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_lr = time.time()\n",
    "    result(all_y_pred_lr, all_y_test_lr, \"Linear Regression\", time_to_train, time_to_predict)\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Linear Regression Completed :) \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Linear Regression\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **3.Logistic Regression**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_logreg = []\n",
    "    all_y_pred_logreg = []\n",
    "    start_cv_logreg = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train  = 0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize Logistic Regression model for this fold\n",
    "        logreg_multi =LogisticRegression(random_state=123, max_iter=5000,solver='newton-cg',multi_class='multinomial')\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Fit the model\n",
    "        start_train = time.time()\n",
    "        logreg_multi.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train - start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = logreg_multi.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        time_to_predict +=  end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_logreg.extend(y_test_fold)\n",
    "        all_y_pred_logreg.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize']=8,8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Logistic Regression\")\n",
    "        pname = method + \"_fold_\"+ str(fold_number) + \"_Logistic_Regression_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_logreg = time.time()\n",
    "    result(all_y_pred_logreg, all_y_test_logreg, \"Logistic Regression\", time_to_train, time_to_predict)\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"LogisticRegression Completed :) \")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"LogisticRegression\")\n",
    "\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **4.K Nearest Neighbor Classifier**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_knn = []\n",
    "    all_y_pred_knn = []\n",
    "    start_cv_knn = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train = 0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize KNN model for this fold\n",
    "        knn = KNeighborsClassifier(8)\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Fit the model\n",
    "        start_train = time.time()\n",
    "        knn.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train - start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = knn.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_knn.extend(y_test_fold)\n",
    "        all_y_pred_knn.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize']=8,8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - KNN\")\n",
    "        pname = method + \"_fold_\"+ str(fold_number) + \"_KNN_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_knn = time.time()\n",
    "    result(all_y_pred_knn, all_y_test_knn, \"KNN\", time_to_train, time_to_predict)\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"KNN Completed :) \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"KNN\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **5.Random Forest Classifier**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_rf = []\n",
    "    all_y_pred_rf = []\n",
    "    start_cv_rf = time.time()\n",
    "    time_to_predict=0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize Random Forest model for this fold\n",
    "        rf = RandomForestClassifier(random_state=24)\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Fit the model\n",
    "        start_train = time.time()\n",
    "        rf.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = rf.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        time_to_predict += end_predict -start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_rf.extend(y_test_fold)\n",
    "        all_y_pred_rf.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize']=8,8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Random Forest\")\n",
    "        pname = method + \"_fold_\"+ str(fold_number) + \"_Random_Forest_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_rf = time.time()\n",
    "    result(all_y_pred_rf, all_y_test_rf, \"Random Forest\", time_to_train, time_to_predict)\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Random forest Completed :) \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Random forest\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **6.Multi Layer Perceptron**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_mlp = []\n",
    "    all_y_pred_mlp = []\n",
    "    start_cv_mlp = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize MLP model for this fold\n",
    "        mlp = MLPClassifier(random_state=24)\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Fit the model\n",
    "        start_train = time.time()\n",
    "        mlp.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = mlp.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_mlp.extend(y_test_fold)\n",
    "        all_y_pred_mlp.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize']=8,8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - MLP\")\n",
    "        pname = method + \"_fold_\"+ str(fold_number) + \"_MLP_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_mlp = time.time()\n",
    "    result(all_y_pred_mlp, all_y_test_mlp, \"MLP\", time_to_train, time_to_predict)\n",
    "\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"MLP Completed :) \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"MLP\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **7.Bagging**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_bagging = []\n",
    "    all_y_pred_bagging = []\n",
    "    start_cv_bagging = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Create a base classifier (Decision Tree)\n",
    "        base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "        # Create a bagging classifier\n",
    "        bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Train the bagging classifier\n",
    "        start_train = time.time()\n",
    "        bagging_classifier.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = bagging_classifier.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        time_to_predict += end_predict-start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_bagging.extend(y_test_fold)\n",
    "        all_y_pred_bagging.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize']=8,8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Bagging\")\n",
    "        pname = method + \"_fold_\"+ str(fold_number) + \"_Bagging_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_bagging = time.time()\n",
    "    result(all_y_pred_bagging, all_y_test_bagging, \"Bagging\", time_to_train, time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Bagging Completed :) \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Bagging\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **8. J48 (C4.5)**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_j48 = []\n",
    "    all_y_pred_j48 = []\n",
    "    start_cv_j48 = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize J48 (C4.5) classifier\n",
    "        classifier_j48 = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Train the J48 (C4.5) classifier\n",
    "        start_train = time.time()\n",
    "        classifier_j48.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = classifier_j48.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_j48.extend(y_test_fold)\n",
    "        all_y_pred_j48.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - J48 (C4.5)\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_J48_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_j48 = time.time()\n",
    "    result(all_y_pred_j48, all_y_test_j48, \"J48\", time_to_train, time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"J48 Completed :) \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"J48\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **9. ANN**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_ann = []\n",
    "    all_y_pred_ann = []\n",
    "    start_cv_ann = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize ANN model\n",
    "        multi_ann = Sequential()\n",
    "        # Adding the input layer and the first hidden layer\n",
    "        multi_ann.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=X_train.shape[1]))\n",
    "        # Adding the second hidden layer\n",
    "        multi_ann.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "        # Adding the output layer\n",
    "        multi_ann.add(Dense(units=10, kernel_initializer='uniform', activation='softmax'))\n",
    "        # Compiling the ANN\n",
    "        multi_ann.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Fitting the ANN to the Training set\n",
    "        start_train = time.time()\n",
    "        history = multi_ann.fit(X_train_fold, y_train_fold, epochs=10, batch_size=50, verbose=0)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = np.argmax(multi_ann.predict(X_test_fold), axis=1)\n",
    "        end_predict = time.time()\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_ann.extend(y_test_fold)\n",
    "        all_y_pred_ann.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - ANN\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_ANN_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_ann = time.time()\n",
    "    result(all_y_pred_ann, all_y_test_ann, \"ANN\", time_to_train, time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"ANN Completed :) \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"ANN\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **10. DNN**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_dnn = []\n",
    "    all_y_pred_dnn = []\n",
    "    start_cv_dnn = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize DNN model\n",
    "        multi_dnn = Sequential()\n",
    "        # Adding the input layer and the first hidden layer\n",
    "        multi_dnn.add(Dense(units=19, kernel_initializer='uniform', activation='relu', input_dim=X_train.shape[1]))\n",
    "        # Adding the second hidden layer\n",
    "        multi_dnn.add(Dense(units=19, kernel_initializer='uniform', activation='relu'))\n",
    "        # Adding the third hidden layer\n",
    "        multi_dnn.add(Dense(units=19, kernel_initializer='uniform', activation='relu'))\n",
    "        # Adding the output layer\n",
    "        multi_dnn.add(Dense(units=10, kernel_initializer='uniform', activation='softmax'))\n",
    "        # Compiling the DNN\n",
    "        multi_dnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Fitting the DNN to the Training set\n",
    "        start_train = time.time()\n",
    "        history = multi_dnn.fit(X_train_fold, y_train_fold, epochs=10, batch_size=50, verbose=0)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = np.argmax(multi_dnn.predict(X_test_fold), axis=1)\n",
    "        end_predict = time.time()\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_dnn.extend(y_test_fold)\n",
    "        all_y_pred_dnn.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - DNN\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_DNN_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_dnn = time.time()\n",
    "    result(all_y_pred_dnn, all_y_test_dnn, \"DNN\", time_to_train, time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"DNN Completed :) \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"DNN\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **12. Gradient Boosting Classifier with k-fold Cross-Validation**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_gb = []\n",
    "    all_y_pred_gb = []\n",
    "    start_cv_gb = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize Gradient Boosting Classifier\n",
    "        multi_gb = GradientBoostingClassifier()\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Train the model\n",
    "        start_train = time.time()\n",
    "        multi_gb.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = multi_gb.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_gb.extend(y_test_fold)\n",
    "        all_y_pred_gb.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Gradient Boosting\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_GradientBoostingClassifier_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_gb = time.time()\n",
    "    result(all_y_pred_gb, all_y_test_gb, \"Gradient Boosting\", time_to_train, time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"GradientBoostingClassifier Completed :) \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"GradientBoostingClassifier\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## ** 13 XGBoost Classifier**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_xgb = []\n",
    "    all_y_pred_xgb = []\n",
    "    start_cv_xgb = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize XGBoost Classifier\n",
    "        xgb_model = XGBClassifier()\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Train the model\n",
    "        start_train = time.time()\n",
    "        xgb_model.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = xgb_model.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_xgb.extend(y_test_fold)\n",
    "        all_y_pred_xgb.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - XGBoost\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_XGBClassifier_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_xgb = time.time()\n",
    "    result(all_y_pred_xgb, all_y_test_xgb, \"XGBoost\", time_to_train, time_to_predict)\n",
    "\n",
    "    #plt.show()\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"XGBClassifier Completed :) \")\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"XGBClassifier\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **14. Gaussian Naive Bayes**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_nb = []\n",
    "    all_y_pred_nb = []\n",
    "    start_cv_nb = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize Gaussian Naive Bayes model\n",
    "        NB_model = GaussianNB()\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Train the model\n",
    "        start_train = time.time()\n",
    "        NB_model.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = NB_model.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_nb.extend(y_test_fold)\n",
    "        all_y_pred_nb.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Gaussian Naive Bayes\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_Gaussian_Naive_Bayes_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_nb = time.time()\n",
    "    result(all_y_pred_nb, all_y_test_nb, \"Gaussian Naive Bayes\", time_to_train, time_to_predict)\n",
    "    #plt.show()\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Gaussian_Naive_Bayes Completed :) \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Gaussian_Naive_Bayes\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **15. Adaptive Gradient Boosting**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_ab = []\n",
    "    all_y_pred_ab = []\n",
    "    start_cv_ab = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize Adaptive Gradient Boosting model\n",
    "        weak_learner = DecisionTreeClassifier(max_leaf_nodes=8)\n",
    "        n_estimators = 300\n",
    "        AB_model = AdaBoostClassifier(\n",
    "            estimator=weak_learner,\n",
    "            n_estimators=n_estimators,\n",
    "            algorithm=\"SAMME\",\n",
    "            random_state=42,\n",
    "        )\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Train the model\n",
    "        start_train = time.time()\n",
    "        AB_model.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = AB_model.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_ab.extend(y_test_fold)\n",
    "        all_y_pred_ab.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Adaptive Gradient Boosting\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_Adaptive_Gradient_Boosting_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_ab = time.time()\n",
    "    result(all_y_pred_ab, all_y_test_ab, \"Adaptive Gradient Boosting\", time_to_train, time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Adaptive Gradient Boosting Completed :) \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Adaptive Gradient Boosting\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **16. Quadratic Discriminant Analysis (QDA)**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_qda = []\n",
    "    all_y_pred_qda = []\n",
    "    start_cv_qda = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Initialize Quadratic Discriminant Analysis (QDA) model\n",
    "        qda_multi = QuadraticDiscriminantAnalysis()\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Train the model\n",
    "        start_train = time.time()\n",
    "        qda_multi.fit(X_train_fold, y_train_fold)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = qda_multi.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_qda.extend(y_test_fold)\n",
    "        all_y_pred_qda.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Quadratic Discriminant Analysis (QDA)\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_QDA_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_qda = time.time()\n",
    "    result(all_y_pred_qda, all_y_test_qda, \"QDA\", time_to_train, time_to_predict)\n",
    "    #plt.show()\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"QDA Completed :) \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"QDA\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **17. Shallow Neural Network (SNN)**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_snn = []\n",
    "    all_y_pred_snn = []\n",
    "    start_cv_snn = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        num_classes =  len(np.unique(y_train))\n",
    "        # Initialize Shallow Neural Network (SNN) model\n",
    "        snn_multi = Sequential()\n",
    "        snn_multi.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "        snn_multi.add(Dense(32, activation='relu'))\n",
    "        snn_multi.add(Dense(20, activation='relu'))\n",
    "        snn_multi.add(Dense(num_classes, activation='softmax'))\n",
    "        snn_multi.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Train the model\n",
    "        start_train = time.time()\n",
    "        history = snn_multi.fit(X_train_fold, y_train_fold, epochs=10, batch_size=50, verbose=0)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = np.argmax(snn_multi.predict(X_test_fold), axis=1)\n",
    "        end_predict = time.time()\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "        # Append metrics to lists\n",
    "        all_y_test_snn.extend(y_test_fold)\n",
    "        all_y_pred_snn.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Shallow Neural Network (SNN)\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_SNN_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_snn = time.time()\n",
    "    result(all_y_pred_snn, all_y_test_snn, \"SNN\", time_to_train, time_to_predict)\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"SNN Completed :)  \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"snn\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **18. Restricted Boltzmann Machine (RBM)**\n",
    "    # Define the RBM class\n",
    "    class RBM(tf.keras.layers.Layer):\n",
    "        def __init__(self, hidden_dim, name=\"rbm\", **kwargs):\n",
    "            super(RBM, self).__init__(name=name, **kwargs)\n",
    "            self.hidden_dim = hidden_dim\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(shape=(input_shape[-1], self.hidden_dim), initializer='uniform', trainable=True, name='weights')\n",
    "            self.h_bias = self.add_weight(shape=(self.hidden_dim,), initializer='zeros', trainable=True, name='h_bias')\n",
    "            self.v_bias = self.add_weight(shape=(input_shape[-1],), initializer='zeros', trainable=True, name='v_bias')\n",
    "        def call(self, inputs):\n",
    "            hidden_prob = tf.nn.sigmoid(tf.matmul(inputs, self.W) + self.h_bias)\n",
    "            hidden_state = self._sample_prob(hidden_prob)\n",
    "            visible_prob = tf.nn.sigmoid(tf.matmul(hidden_state, tf.transpose(self.W)) + self.v_bias)\n",
    "            return visible_prob, hidden_state\n",
    "        def _sample_prob(self, probs):\n",
    "            return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_rbm = []\n",
    "    all_y_pred_rbm = []\n",
    "    start_cv_rbm = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        input_data = Input(shape=(X_train.shape[1],))\n",
    "        rbm1_visible, rbm1_hidden = RBM(hidden_dim=128, name=f\"rbm1_fold_{fold_number}\")(input_data)\n",
    "        rbm2_visible, rbm2_hidden = RBM(hidden_dim=64, name=f\"rbm2_fold_{fold_number}\")(rbm1_hidden)\n",
    "        rbm3_visible, rbm3_hidden = RBM(hidden_dim=32, name=f\"rbm3_fold_{fold_number}\")(rbm2_hidden)\n",
    "        rbm6_visible, rbm6_hidden = RBM(hidden_dim=64, name=f\"rbm6_fold_{fold_number}\")(rbm3_hidden)\n",
    "        classifier_output = Dense(num_classes, activation='softmax', name=f'classifier_fold_{fold_number}')(rbm6_hidden)\n",
    "        model = tf.keras.Model(inputs=input_data, outputs=classifier_output)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        start_train = time.time()\n",
    "        model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=50, verbose=0)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = np.argmax(model.predict(X_test_fold), axis=1)\n",
    "        end_predict = time.time()\n",
    "\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_rbm.extend(y_test_fold)\n",
    "        all_y_pred_rbm.extend(y_pred_fold)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Restricted Boltzmann Machine (RBM)\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_RBM_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_rbm = time.time()\n",
    "    result(all_y_pred_rbm, all_y_test_rbm, \"RBM\", time_to_train, time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"RBM Completed :)  \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"RBM\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **19. LSTM**\n",
    "\n",
    "    # reloading as many transformations on X,Y causing errors for lstm code\n",
    "\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_lstm = []\n",
    "    all_y_pred_lstm = []\n",
    "    start_cv_lstm = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        num_classes = len(np.unique(y_train))\n",
    "\n",
    "        # Convert DataFrame to NumPy array and reshape input data for LSTM\n",
    "        X_train_array_multi = X_train.iloc[train_index].to_numpy()\n",
    "        X_test_array_multi = X_train.iloc[test_index].to_numpy()\n",
    "        X_train_reshaped_multi = X_train_array_multi.reshape((X_train_array_multi.shape[0], X_train_array_multi.shape[1], 1))\n",
    "        X_test_reshaped_multi = X_test_array_multi.reshape((X_test_array_multi.shape[0], X_test_array_multi.shape[1], 1))\n",
    "\n",
    "        # Define the LSTM model\n",
    "        rnn_multi = Sequential()\n",
    "        rnn_multi.add(LSTM(128, input_shape=(X_train_reshaped_multi.shape[1], X_train_reshaped_multi.shape[2])))\n",
    "        rnn_multi.add(Dense(32, activation='relu'))\n",
    "        rnn_multi.add(Dense(num_classes, activation='softmax'))\n",
    "        rnn_multi.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the LSTM model\n",
    "        start_train = time.time()\n",
    "        rnn_multi.fit(X_train_reshaped_multi, y_train.iloc[train_index], validation_data=(X_test_reshaped_multi, y_train.iloc[test_index]), epochs=10, batch_size=50, verbose=0)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = np.argmax(rnn_multi.predict(X_test_reshaped_multi), axis=1)\n",
    "        end_predict = time.time()\n",
    "\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_lstm.extend(y_train.iloc[test_index])\n",
    "        all_y_pred_lstm.extend(y_pred_fold)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_train.iloc[test_index], y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - LSTM\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_LSTM_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_lstm = time.time()\n",
    "    result(all_y_pred_lstm, all_y_test_lstm, \"LSTM\",time_to_train , time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"lstm Completed :)  \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"lstm\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **20. Reconstruction Neural Networks**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_recon = []\n",
    "    all_y_pred_recon = []\n",
    "    start_cv_recon = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        num_classes = len(np.unique(y_train))\n",
    "\n",
    "        # Assuming y_train_multi is one-hot encoded\n",
    "        y_train_multi_onehot = tf.keras.utils.to_categorical(y_train.iloc[train_index], num_classes=num_classes)\n",
    "        y_test_multi_onehot = tf.keras.utils.to_categorical(y_train.iloc[test_index], num_classes=num_classes)\n",
    "\n",
    "        # Define model architecture\n",
    "        input_dim = X_train.shape[1]\n",
    "        encoding_dim = 32  # Choose appropriate dimensionality\n",
    "        latent_dim = 2  # Dimensionality of the latent space\n",
    "\n",
    "        # Encoder\n",
    "        input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "        hidden = tf.keras.layers.Dense(64, activation='relu')(input_layer)\n",
    "        z_mean = tf.keras.layers.Dense(latent_dim)(hidden)\n",
    "        z_log_var = tf.keras.layers.Dense(latent_dim)(hidden)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "        z = tf.keras.layers.Lambda(sampling,output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "        # Decoder\n",
    "        decoder_hidden = tf.keras.layers.Dense(64, activation='relu')(z)\n",
    "        output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(decoder_hidden)\n",
    "\n",
    "        # Define VAE model\n",
    "        vae = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        # Compile model\n",
    "        vae.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the VAE model\n",
    "        start_train = time.time()\n",
    "        vae.fit(X_train.iloc[train_index], y_train_multi_onehot, epochs=10, batch_size=50, shuffle=True, verbose=0)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = np.argmax(vae.predict(X_train.iloc[test_index]), axis=1)\n",
    "        end_predict = time.time()\n",
    "\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_recon.extend(y_train.iloc[test_index])\n",
    "        all_y_pred_recon.extend(y_pred_fold)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_train.iloc[test_index], y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Reconstruction Neural Network\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_reconstruction_NN_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_recon = time.time()\n",
    "    result(all_y_pred_recon, all_y_test_recon, \"reconstruction_NN\", time_to_train, time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"reconstruction neural networks, Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"reconstruction neural networks\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **22. DANN with k-fold Cross-Validation**\n",
    "\n",
    "    def build_dann_model(input_shape, num_classes, lambda_val=1e-3):\n",
    "        input_layer = Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "        # Feature extractor\n",
    "        shared_layer = Dense(128, activation='relu')(input_layer)\n",
    "        shared_layer = Dropout(0.5)(shared_layer)\n",
    "\n",
    "        # Source classifier\n",
    "        source_classifier = Dense(num_classes, activation='softmax', name='source_classifier')(shared_layer)\n",
    "\n",
    "        # Domain classifier\n",
    "        domain_classifier = Dense(1, activation='sigmoid', name='domain_classifier')(shared_layer)\n",
    "\n",
    "        # Combined model\n",
    "        model = Model(inputs=input_layer, outputs=[source_classifier, domain_classifier])\n",
    "\n",
    "        # Domain adversarial loss\n",
    "        def domain_adversarial_loss(y_true, y_pred):\n",
    "            return K.mean(K.binary_crossentropy(y_true, y_pred))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "                    loss={'source_classifier': 'categorical_crossentropy', 'domain_classifier': domain_adversarial_loss},\n",
    "                    loss_weights={'source_classifier': 1.0, 'domain_classifier': lambda_val},\n",
    "                    metrics={'source_classifier': 'accuracy'})\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_dann = []\n",
    "    all_y_pred_dann = []\n",
    "    start_cv_dann = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Convert class vectors to binary class matrices\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        y_train_categorical = tf.keras.utils.to_categorical(y_train.iloc[train_index], num_classes)\n",
    "        y_test_categorical = tf.keras.utils.to_categorical(y_train.iloc[test_index], num_classes)\n",
    "\n",
    "        # Build and train DANN model for each fold\n",
    "        input_shape = (X_train.shape[1],)\n",
    "        lambda_val = 1e-3  # Trade-off parameter for domain adversarial loss\n",
    "        dann_model = build_dann_model(input_shape, num_classes, lambda_val)\n",
    "\n",
    "        # Training phase\n",
    "        start_train = time.time()\n",
    "        dann_model.fit(X_train.iloc[train_index],\n",
    "                    {'source_classifier': y_train_categorical, 'domain_classifier': np.zeros((len(train_index), 1))},\n",
    "                    epochs=10, batch_size=64,\n",
    "                    validation_data=(X_train.iloc[test_index],\n",
    "                                        {'source_classifier': y_test_categorical,\n",
    "                                        'domain_classifier': np.ones((len(test_index), 1))}))\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Prediction phase\n",
    "        X_test_fold = X_train.iloc[test_index]  # Use iloc to access test fold\n",
    "        start_predict = time.time()\n",
    "        predictions = dann_model.predict(X_test_fold)\n",
    "        source_classifier_predictions = predictions[0]\n",
    "        y_pred_fold = np.argmax(source_classifier_predictions, axis=1)\n",
    "        end_predict = time.time()\n",
    "\n",
    "        y_test = y_train.iloc[test_index]\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_dann.extend(y_test)\n",
    "        all_y_pred_dann.extend(y_pred_fold)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - DANN\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_DANN_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_dann = time.time()\n",
    "    result(all_y_pred_dann, all_y_test_dann, \"DANN\", time_to_train, time_to_predict)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"DANN Completed :)  \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"DANN\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **23.Deep brief networks (DBNs)**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_dbn = []\n",
    "    all_y_pred_dbn = []\n",
    "    start_cv_dbn = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Number of classes\n",
    "        num_classes = len(np.unique(y_train))\n",
    "\n",
    "        # Create a pipeline with BernoulliRBM and MLPClassifier\n",
    "        rbm = BernoulliRBM(n_components=64, learning_rate=0.01, n_iter=20, random_state=42, verbose=True)\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(128,), max_iter=10, random_state=52)\n",
    "        dbn_model = Pipeline(steps=[('rbm', rbm), ('mlp', mlp)])\n",
    "\n",
    "        # Training phase\n",
    "        start_train = time.time()\n",
    "        dbn_model.fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Prediction phase\n",
    "        X_test_fold = X_train.iloc[test_index]\n",
    "        y_test_fold = y_train.iloc[test_index]\n",
    "        start_predict = time.time()\n",
    "        y_pred_fold = dbn_model.predict(X_test_fold)\n",
    "        end_predict = time.time()\n",
    "\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_dbn.extend(y_test_fold)\n",
    "        all_y_pred_dbn.extend(y_pred_fold)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - DBN\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_DBN_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_dbn = time.time()\n",
    "    result(all_y_pred_dbn, all_y_test_dbn, \"DBN\", time_to_train, time_to_predict)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"DBNs\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **24. Deep Boltzmann Machines (DBMs)** with k-fold Cross-Validation\n",
    "    # Build a simple Restricted Boltzmann Machine (RBM) using TensorFlow\n",
    "    class RBM(tf.Module):\n",
    "        def __init__(self, visible_dim, hidden_dim, learning_rate=0.01):\n",
    "            self.visible_dim = visible_dim\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.learning_rate = learning_rate\n",
    "\n",
    "            # Initialize weights and biases\n",
    "            self.W = tf.Variable(tf.random.normal([visible_dim, hidden_dim], stddev=0.01, dtype=tf.float32))\n",
    "            self.b_visible = tf.Variable(tf.zeros([visible_dim], dtype=tf.float32))\n",
    "            self.b_hidden = tf.Variable(tf.zeros([hidden_dim], dtype=tf.float32))\n",
    "\n",
    "        def _softmax(self, x):\n",
    "            exp_x = tf.exp(x)\n",
    "            return exp_x / tf.reduce_sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "        def sample_hidden(self, visible_prob):\n",
    "            hidden_prob = self._softmax(tf.matmul(visible_prob, self.W) + self.b_hidden)\n",
    "            return tf.nn.relu(tf.sign(hidden_prob - tf.random.uniform(tf.shape(hidden_prob))))\n",
    "\n",
    "        def sample_visible(self, hidden_prob):\n",
    "            visible_prob = self._softmax(tf.matmul(hidden_prob, tf.transpose(self.W)) + self.b_visible)\n",
    "            return tf.nn.relu(tf.sign(visible_prob - tf.random.uniform(tf.shape(visible_prob))))\n",
    "\n",
    "        def contrastive_divergence(self, x, k=1):\n",
    "            visible = x\n",
    "            for _ in range(k):\n",
    "                hidden = self.sample_hidden(visible)\n",
    "                visible = self.sample_visible(hidden)\n",
    "\n",
    "            positive_hidden = self._softmax(tf.matmul(x, self.W) + self.b_hidden)\n",
    "            negative_hidden = self._softmax(tf.matmul(visible, self.W) + self.b_hidden)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.W.assign_add(self.learning_rate * (tf.matmul(tf.transpose(x), positive_hidden) -\n",
    "                                                    tf.matmul(tf.transpose(visible), negative_hidden)))\n",
    "            self.b_visible.assign_add(self.learning_rate * tf.reduce_mean(x - visible, axis=0))\n",
    "            self.b_hidden.assign_add(self.learning_rate * tf.reduce_mean(positive_hidden - negative_hidden, axis=0))\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_dbm = []\n",
    "    all_y_pred_dbm = []\n",
    "    start_cv_dbm = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index].values, X_train.iloc[test_index].values\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index].values, y_train.iloc[test_index].values\n",
    "\n",
    "        # Number of visible and hidden units\n",
    "        visible_dim = X_train_fold.shape[1]\n",
    "        hidden_dim1 = 64\n",
    "        hidden_dim2 = 32\n",
    "\n",
    "        # Create RBMs for each layer\n",
    "        rbm1 = RBM(visible_dim, hidden_dim1)\n",
    "        rbm2 = RBM(hidden_dim1, hidden_dim2)\n",
    "\n",
    "        # Training RBMs\n",
    "        num_epochs = 5\n",
    "        batch_size = 32\n",
    "        start = time.time()\n",
    "        # Training first RBM\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(0, len(X_train_fold), batch_size):\n",
    "                batch_data = X_train_fold[i:i+batch_size]\n",
    "                rbm1.contrastive_divergence(tf.cast(batch_data, dtype=tf.float32))\n",
    "\n",
    "        # Getting hidden layer representation from the first RBM\n",
    "        hidden1_representation = tf.nn.relu(tf.sign(rbm1.sample_hidden(tf.cast(X_train_fold, dtype=tf.float32))))\n",
    "\n",
    "        # Training second RBM using the hidden layer representation from the first RBM\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(0, len(hidden1_representation), batch_size):\n",
    "                batch_data = hidden1_representation[i:i+batch_size]\n",
    "                rbm2.contrastive_divergence(batch_data)\n",
    "\n",
    "        # Getting hidden layer representation from the second RBM\n",
    "        hidden2_representation = tf.nn.relu(tf.sign(rbm2.sample_hidden(hidden1_representation)))\n",
    "\n",
    "        # Fine-tuning for classification\n",
    "        num_classes = len(np.unique(y_train_fold))\n",
    "        dbm_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(hidden_dim1, activation='relu'),\n",
    "            tf.keras.layers.Dense(hidden_dim2, activation='relu'),\n",
    "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        dbm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        start_train = time.time()\n",
    "        dbm_model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=50, shuffle=True, verbose=0)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "\n",
    "        # Predict on the test set\n",
    "        start_predict = time.time()\n",
    "        y_pred_probabilities = dbm_model.predict(X_test_fold)\n",
    "        y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "        end_predict = time.time()\n",
    "\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_dbm.extend(y_test_fold)\n",
    "        all_y_pred_dbm.extend(y_pred)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - DBM\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_DBM_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_dbm = time.time()\n",
    "    result(all_y_pred_dbm, all_y_test_dbm, \"DBM\", time_to_train, time_to_predict)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"DBMs\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **25.DEEP AUTO ENCODERS(DA)**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_da = []\n",
    "    all_y_pred_da = []\n",
    "    start_cv_da = time.time()\n",
    "    time_to_predict = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Define the autoencoder model\n",
    "        autoencoder = Sequential()\n",
    "\n",
    "        # Encoder\n",
    "        autoencoder.add(Dense(128, activation='relu', input_shape=(X_train_fold.shape[1],)))\n",
    "        autoencoder.add(Dense(64, activation='relu'))\n",
    "        autoencoder.add(Dense(32, activation='relu'))\n",
    "\n",
    "        # Decoder\n",
    "        autoencoder.add(Dense(64, activation='relu'))\n",
    "        autoencoder.add(Dense(128, activation='relu'))\n",
    "        autoencoder.add(Dense(X_train_fold.shape[1], activation='linear'))\n",
    "\n",
    "        # Compile the autoencoder\n",
    "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        # Train the autoencoder\n",
    "        autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=50, verbose=0)\n",
    "\n",
    "        # Add a classification head on top of the trained autoencoder\n",
    "        da_model = Sequential()\n",
    "        da_model.add(autoencoder.layers[0])  # Add encoder layers\n",
    "        da_model.add(autoencoder.layers[1])\n",
    "        da_model.add(autoencoder.layers[2])\n",
    "        da_model.add(Dense(num_classes, activation='softmax'))  # Adjust output layer for multiple classes\n",
    "\n",
    "        # Compile the classification model\n",
    "        da_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Convert labels to one-hot encoding\n",
    "        y_train_fold_onehot = to_categorical(y_train_fold, num_classes=num_classes)\n",
    "        y_test_fold_onehot = to_categorical(y_test_fold, num_classes=num_classes)\n",
    "\n",
    "        # Train the classification model using the encoded representations\n",
    "        start_train = time.time()\n",
    "        history = da_model.fit(X_train_fold, y_train_fold_onehot, epochs=10, batch_size=32, shuffle=True, verbose=0)\n",
    "        end_train = time.time()\n",
    "        time_to_train += end_train -start_train\n",
    "        # Predict on the test set\n",
    "        start_predict = time.time()\n",
    "        y_pred_probabilities = da_model.predict(X_test_fold)\n",
    "        y_pred_fold = np.argmax(y_pred_probabilities, axis=1)\n",
    "        end_predict = time.time()\n",
    "\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict += end_predict - start_predict\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_da.extend(y_test_fold)\n",
    "        all_y_pred_da.extend(y_pred_fold)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - DA\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_DA_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_da = time.time()\n",
    "    result(all_y_pred_da, all_y_test_da, \"DA\", time_to_train, time_to_predict)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"DEEP AUTO ENCODERS\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **26. PassiveAggressiveClassifier with k-fold Cross-Validation**\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_passive = []\n",
    "    all_y_pred_passive = []\n",
    "\n",
    "    start_cv_passive = time.time()\n",
    "    time_to_predict_passive = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets\n",
    "        X_train_fold_passive, X_test_fold_passive = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold_passive, y_test_fold_passive = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Initialize PassiveAggressiveClassifier for each fold\n",
    "        model_passive = PassiveAggressiveClassifier(max_iter=1000, random_state=0, tol=1e-3)\n",
    "\n",
    "        # Train the model\n",
    "        start_train_passive = time.time()\n",
    "        model_passive.fit(X_train_fold_passive, y_train_fold_passive)\n",
    "        end_train_passive = time.time()\n",
    "        time_to_train += end_train_passive -start_train_passive\n",
    "        # Predict on the test set\n",
    "        start_predict_passive = time.time()\n",
    "        y_pred_passive = model_passive.predict(X_test_fold_passive)\n",
    "        end_predict_passive = time.time()\n",
    "        time_to_predict_passive += end_predict_passive - start_predict_passive\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_passive.extend(y_test_fold_passive)\n",
    "        all_y_pred_passive.extend(y_pred_passive)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm_passive = confusion_matrix(y_test_fold_passive, y_pred_passive)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_passive = ConfusionMatrixDisplay(confusion_matrix=cm_passive)\n",
    "        disp_passive.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - PassiveAggressiveClassifier\")\n",
    "        pname_passive = method + \"_fold_\" + str(fold_number) + \"_PassiveAggressiveClassifier_confusion_matrix.png\"\n",
    "        plt.savefig(pname_passive)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_passive = time.time()\n",
    "    result(all_y_pred_passive, all_y_test_passive, \"PassiveAggressiveClassifier\", time_to_train, time_to_predict_passive)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"PassiveAggressiveClassifier\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **27. RidgeClassifier with k-fold Cross-Validation**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_ridge = []\n",
    "    all_y_pred_ridge = []\n",
    "    start_cv_ridge = time.time()\n",
    "    time_to_predict_ridge = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train.values, y_train.values), 1):\n",
    "        # Split data into train and test sets\n",
    "        X_train_fold_ridge, X_test_fold_ridge = X_train.values[train_index], X_train.values[test_index]\n",
    "        y_train_fold_ridge, y_test_fold_ridge = y_train.values[train_index], y_train.values[test_index]\n",
    "\n",
    "        # Initialize RidgeClassifier for each fold\n",
    "        model_ridge = RidgeClassifier()\n",
    "\n",
    "        # Train the model\n",
    "        start_train_ridge = time.time()\n",
    "        model_ridge.fit(X_train_fold_ridge, y_train_fold_ridge)\n",
    "        end_train_ridge = time.time()\n",
    "        time_to_train += end_train_ridge -start_train_ridge\n",
    "        # Predict    on the test set\n",
    "        start_predict_ridge = time.time()\n",
    "        y_pred_ridge = model_ridge.predict(X_test_fold_ridge)\n",
    "        end_predict_ridge = time.time()\n",
    "\n",
    "        time_to_predict_ridge += end_predict_ridge - start_predict_ridge\n",
    "        # Append metrics to lists\n",
    "        all_y_test_ridge.extend(y_test_fold_ridge)\n",
    "        all_y_pred_ridge.extend(y_pred_ridge)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm_ridge = confusion_matrix(y_test_fold_ridge, y_pred_ridge)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_ridge = ConfusionMatrixDisplay(confusion_matrix=cm_ridge)\n",
    "        disp_ridge.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - RidgeClassifier\")\n",
    "        pname_ridge = method +\"_fold_\"+ str(fold_number) + \"_RidgeClassifier_confusion_matrix.png\"\n",
    "        plt.savefig(pname_ridge)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_ridge = time.time()\n",
    "    result(all_y_pred_ridge, all_y_test_ridge, \"RidgeClassifier\", time_to_train, time_to_predict_ridge)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"RidgeClassifier\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **28. NearestCentroid with k-fold Cross-Validation, Time to Predict, and Confusion Matrix**\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_nc = []\n",
    "    all_y_pred_nc = []\n",
    "    start_cv_nc = time.time()\n",
    "    total_time_to_predict_nc = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets\n",
    "        X_train_fold_nc, X_test_fold_nc = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold_nc, y_test_fold_nc = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Initialize NearestCentroid model for each fold\n",
    "        model_nc = NearestCentroid()\n",
    "\n",
    "        # Start time for training\n",
    "        start_train_nc = time.time()\n",
    "\n",
    "        # Train the model\n",
    "        model_nc.fit(X_train_fold_nc, y_train_fold_nc)\n",
    "\n",
    "        # End time for training\n",
    "        end_train_nc = time.time()\n",
    "        time_to_train += end_train_nc - start_train_nc\n",
    "        # Start time for prediction\n",
    "        start_predict_nc = time.time()\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred_fold_nc = model_nc.predict(X_test_fold_nc)\n",
    "\n",
    "        # End time for prediction\n",
    "        end_predict_nc = time.time()\n",
    "\n",
    "        # Calculate time taken for prediction\n",
    "        time_to_predict_nc = end_predict_nc - start_predict_nc\n",
    "        total_time_to_predict_nc += time_to_predict_nc\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_nc.extend(y_test_fold_nc)\n",
    "        all_y_pred_nc.extend(y_pred_fold_nc)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm_nc = confusion_matrix(y_test_fold_nc, y_pred_fold_nc)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_nc = ConfusionMatrixDisplay(confusion_matrix=cm_nc)\n",
    "        disp_nc.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - NearestCentroid\")\n",
    "        pname_nc = method+\"_fold_\" + str(fold_number) + \"_NearestCentroid_confusion_matrix.png\"\n",
    "        plt.savefig(pname_nc)\n",
    "        #plt.show()\n",
    "\n",
    "    # End time for k-fold cross-validation\n",
    "    end_cv_nc = time.time()\n",
    "    # Calculate overall performance metrics\n",
    "    result(all_y_pred_nc, all_y_test_nc, \"NearestCentroid\", time_to_train, total_time_to_predict_nc)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"NearestCentroid\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **29. Cost Sensitive Logistic Regression (CSLR) with k-fold Cross-Validation and Time to Predict**\n",
    "\n",
    "    def get_sample_weight(cost_matrix, y_tru):\n",
    "        y_true = np.array(y_tru)\n",
    "        num_samples = len(y_true)\n",
    "        sample_weights = np.zeros(num_samples)\n",
    "        for i in range(num_samples):\n",
    "            true_class = y_true[i]\n",
    "            for j in range(len(cost_matrix)):\n",
    "                if j != true_class:\n",
    "                    sample_weights[i] += cost_matrix[true_class, j]\n",
    "        return sample_weights\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_cslr = []\n",
    "    all_y_pred_cslr = []\n",
    "    # Define cost matrix for CSLR\n",
    "    cost_matrix = np.array([[0, 1, 2, 3, 4, 5, 6, 7, 8],    # Costs for misclassifying class 0\n",
    "                            [1, 0, 1, 2, 3, 4, 5, 6, 7],    # Costs for misclassifying class 1\n",
    "                            [2, 1, 0, 1, 2, 3, 4, 5, 6],    # Costs for misclassifying class 2\n",
    "                            [3, 2, 1, 0, 1, 2, 3, 4, 5],    # Costs for misclassifying class 3\n",
    "                            [4, 3, 2, 1, 0, 1, 2, 3, 4],    # Costs for misclassifying class 4\n",
    "                            [5, 4, 3, 2, 1, 0, 1, 2, 3],    # Costs for misclassifying class 5\n",
    "                            [6, 5, 4, 3, 2, 1, 0, 1, 2],    # Costs for misclassifying class 6\n",
    "                            [7, 6, 5, 4, 3, 2, 1, 0, 1],    # Costs for misclassifying class 7\n",
    "                            [8, 7, 6, 5, 4, 3, 2, 1, 0]])   # Costs for misclassifying class 8\n",
    "\n",
    "    start_cv_cslr = time.time()\n",
    "    total_time_to_predict_fold_cslr = 0\n",
    "    time_to_train = 0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets\n",
    "        X_train_fold_cslr, X_test_fold_cslr = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold_cslr, y_test_fold_cslr = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Get sample weights for cost-sensitive learning\n",
    "        sample_weights_fold_cslr = get_sample_weight(cost_matrix, y_train_fold_cslr)\n",
    "\n",
    "        # Initialize Logistic Regression model for each fold\n",
    "        model_cslr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "        # Start time for training\n",
    "        start_train_fold_cslr = time.time()\n",
    "\n",
    "        # Train the model\n",
    "        model_cslr.fit(X_train_fold_cslr, y_train_fold_cslr, sample_weight=sample_weights_fold_cslr)\n",
    "\n",
    "        # End time for training\n",
    "        end_train_fold_cslr = time.time()\n",
    "\n",
    "        time_to_train +=  end_train_fold_cslr - start_train_fold_cslr\n",
    "        # Start time for prediction\n",
    "        start_predict_fold_cslr = time.time()\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred_fold_cslr = model_cslr.predict(X_test_fold_cslr)\n",
    "\n",
    "        # End time for prediction\n",
    "        end_predict_fold_cslr = time.time()\n",
    "\n",
    "        # Calculate time taken for prediction in this fold\n",
    "        time_to_predict_fold_cslr = end_predict_fold_cslr - start_predict_fold_cslr\n",
    "        total_time_to_predict_fold_cslr += time_to_predict_fold_cslr\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_cslr.extend(y_test_fold_cslr)\n",
    "        all_y_pred_cslr.extend(y_pred_fold_cslr)\n",
    "\n",
    "        # Generate confusion matrix and display\n",
    "        cm_cslr = confusion_matrix(y_test_fold_cslr, y_pred_fold_cslr)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_cslr = ConfusionMatrixDisplay(confusion_matrix=cm_cslr)\n",
    "        disp_cslr.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - CSLR\")\n",
    "        pname_cslr = method +\"_fold_\" + str(fold_number) + \"_CSLR__confusion_matrix.png\"\n",
    "        plt.savefig(pname_cslr)\n",
    "        #plt.show()\n",
    "\n",
    "    # End time for k-fold cross-validation\n",
    "    end_cv_cslr = time.time()\n",
    "    result(all_y_pred_cslr, all_y_test_cslr, \"CSLR\", time_to_train, total_time_to_predict_fold_cslr)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"CSLR\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **30. Cost-sensitive Bagging Classifier (CSBC) with k-fold Cross-Validation**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_csbc = []\n",
    "    all_y_pred_csbc = []\n",
    "    start_cv_csbc = time.time()\n",
    "    time_to_predict_fold_csbc = 0\n",
    "    time_to_train = 0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets\n",
    "        X_train_fold_csbc, X_test_fold_csbc = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold_csbc, y_test_fold_csbc = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Step 1: Compute class weights for the fold\n",
    "        class_weights_fold = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_fold_csbc), y=y_train_fold_csbc)\n",
    "\n",
    "        # Step 2: Initialize the base estimator and BaggingClassifier for the fold\n",
    "        base_estimator = DecisionTreeClassifier(max_depth=5)\n",
    "        bagging_model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
    "\n",
    "        # Step 3: Train the model on the fold\n",
    "        start_train_fold_csbc = time.time()\n",
    "        bagging_model.fit(X_train_fold_csbc, y_train_fold_csbc)\n",
    "        end_train_fold_csbc = time.time()\n",
    "        time_to_train += end_train_fold_csbc - start_train_fold_csbc\n",
    "        # Step 4: Predict on the test set for the fold\n",
    "        start_predict_fold_csbc = time.time()\n",
    "        y_pred_fold_csbc = bagging_model.predict(X_test_fold_csbc)\n",
    "        end_predict_fold_csbc = time.time()\n",
    "\n",
    "        time_to_predict_fold_csbc += end_predict_fold_csbc - start_predict_fold_csbc\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_csbc.extend(y_test_fold_csbc)\n",
    "        all_y_pred_csbc.extend(y_pred_fold_csbc)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_csbc = confusion_matrix(y_test_fold_csbc, y_pred_fold_csbc)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_csbc = ConfusionMatrixDisplay(confusion_matrix=cm_csbc)\n",
    "        disp_csbc.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - CSBC\")\n",
    "        pname_csbc = method + \"_fold_\" + str(fold_number) + \"_CSBC_confusion_matrix.png\"\n",
    "        plt.savefig(pname_csbc)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_csbc = time.time()\n",
    "    result(all_y_pred_csbc, all_y_test_csbc, \"CSBC\", time_to_train, time_to_predict_fold_csbc)\n",
    "\n",
    "    from lightgbm import LGBMClassifier\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"CSBC\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **31. LightGBM with k-fold Cross-Validation**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_lgbm = []\n",
    "    all_y_pred_lgbm = []\n",
    "    start_cv_lgbm = time.time()\n",
    "    time_to_predict_fold_lgbm = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets\n",
    "        X_train_fold_lgbm, X_test_fold_lgbm = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold_lgbm, y_test_fold_lgbm = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Initialize LightGBM Classifier\n",
    "        lgbm = LGBMClassifier()\n",
    "\n",
    "        # Train the model on the fold\n",
    "        start_train_fold_lgbm = time.time()\n",
    "        lgbm.fit(X_train_fold_lgbm, y_train_fold_lgbm)\n",
    "        end_train_fold_lgbm = time.time()\n",
    "        time_to_train += end_train_fold_lgbm  - start_train_fold_lgbm\n",
    "        # Predict on the test set for the fold\n",
    "        start_predict_fold_lgbm = time.time()\n",
    "        y_pred_fold_lgbm = lgbm.predict(X_test_fold_lgbm)\n",
    "        end_predict_fold_lgbm = time.time()\n",
    "\n",
    "        time_to_predict_fold_lgbm += end_predict_fold_lgbm - start_predict_fold_lgbm\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_lgbm.extend(y_test_fold_lgbm)\n",
    "        all_y_pred_lgbm.extend(y_pred_fold_lgbm)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_lgbm = confusion_matrix(y_test_fold_lgbm, y_pred_fold_lgbm)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_lgbm = ConfusionMatrixDisplay(confusion_matrix=cm_lgbm)\n",
    "        disp_lgbm.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - LightGBM\")\n",
    "        pname_lgbm = method + \"_fold_\" + str(fold_number) + \"_LightGBM_confusion_matrix.png\"\n",
    "        plt.savefig(pname_lgbm)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_lgbm = time.time()\n",
    "    result(all_y_pred_lgbm, all_y_test_lgbm, \"LightGBM\", time_to_train, time_to_predict_fold_lgbm)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"LightGBM\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **32. LinearDiscriminantAnalysis (LDA) with k-fold Cross-Validation\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_lda = []\n",
    "    all_y_pred_lda = []\n",
    "    start_cv_lda = time.time()\n",
    "    time_to_predict_fold_lda = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets\n",
    "        X_train_fold_lda, X_test_fold_lda = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold_lda, y_test_fold_lda = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Apply Linear Discriminant Analysis (LDA) for dimensionality reduction\n",
    "        lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "        X_train_fold_lda = lda.fit_transform(X_train_fold_lda, y_train_fold_lda)\n",
    "        X_test_fold_lda = lda.transform(X_test_fold_lda)\n",
    "\n",
    "        # Train Random Forest Classifier on the transformed features\n",
    "        classifier_lda = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "        # Train the model on the fold\n",
    "        start_train_fold_lda = time.time()\n",
    "        classifier_lda.fit(X_train_fold_lda, y_train_fold_lda)\n",
    "        end_train_fold_lda = time.time()\n",
    "        time_to_train += end_train_fold_lda -start_train_fold_lda\n",
    "        # Predict on the test set for the fold\n",
    "        start_predict_fold_lda = time.time()\n",
    "        y_pred_fold_lda = classifier_lda.predict(X_test_fold_lda)\n",
    "        end_predict_fold_lda = time.time()\n",
    "\n",
    "        time_to_predict_fold_lda += end_predict_fold_lda - start_predict_fold_lda\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_lda.extend(y_test_fold_lda)\n",
    "        all_y_pred_lda.extend(y_pred_fold_lda)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_lda = confusion_matrix(y_test_fold_lda, y_pred_fold_lda)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_lda = ConfusionMatrixDisplay(confusion_matrix=cm_lda)\n",
    "        disp_lda.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - LDA\")\n",
    "        pname_lda = method + \"_fold_\" + str(fold_number) + \"_LDA_confusion_matrix.png\"\n",
    "        plt.savefig(pname_lda)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_lda = time.time()\n",
    "    result(all_y_pred_lda, all_y_test_lda, \"LDA\", time_to_train, time_to_predict_fold_lda)\n",
    "\n",
    "    # **MULTI-CLASS CLASSIFICATION**\n",
    "    # **Data Splitting**\n",
    "    # reloading as many transformations on X,Y causing errors for gru code\n",
    "    X_train = train_data.drop(columns=['label'],axis=1)\n",
    "    X_test = test_data.drop(columns=['label'],axis=1)\n",
    "    y_train = train_data['label']\n",
    "    y_test = test_data['label']\n",
    "    X_train = pd.concat([X_train, X_test], axis=0)\n",
    "    y_train = pd.concat([y_train, y_test], axis=0)\n",
    "\n",
    "    # # 10 DATA COL EACH CLASS\n",
    "    # # Get unique classes\n",
    "    X_train,temp1,y_train,temp2 = train_test_split(X_train,y_train,train_size=0.1, random_state=7)\n",
    "\n",
    "    # Reset indices of X_train and y_train\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"LDA\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **33. GRU with k-fold Cross-Validation**\n",
    "    num_classes =  len(np.unique(y_train))\n",
    "    X_train_array_multi = X_train.to_numpy()\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_gru = []\n",
    "    all_y_pred_gru = []\n",
    "    start_cv_gru = time.time()\n",
    "    time_to_predict_fold = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Convert fold data to numpy arrays and reshape for GRU input\n",
    "        X_train_fold_array = X_train_fold.to_numpy().reshape((X_train_fold.shape[0], X_train_fold.shape[1], 1))\n",
    "        X_test_fold_array = X_test_fold.to_numpy().reshape((X_test_fold.shape[0], X_test_fold.shape[1], 1))\n",
    "\n",
    "        # Define and compile the GRU model\n",
    "        rnn_fold = Sequential([\n",
    "            GRU(128, input_shape=(X_train_fold_array.shape[1], X_train_fold_array.shape[2])),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        rnn_fold.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model on the fold\n",
    "        start_train_fold = time.time()\n",
    "        rnn_fold.fit(X_train_fold_array, y_train_fold, epochs=10, batch_size=50, verbose=0)\n",
    "        end_train_fold = time.time()\n",
    "        time_to_train += end_train_fold -start_train_fold\n",
    "        # Predict on the test set for the fold\n",
    "        start_predict_fold = time.time()\n",
    "        y_pred_fold = np.argmax(rnn_fold.predict(X_test_fold_array), axis=1)\n",
    "        end_predict_fold = time.time()\n",
    "\n",
    "        time_to_predict_fold += end_predict_fold - start_predict_fold\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_gru.extend(y_test_fold)\n",
    "        all_y_pred_gru.extend(y_pred_fold)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - GRU\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_GRU_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_gru = time.time()\n",
    "    val_accuracy_gru = accuracy_score(all_y_test_gru, all_y_pred_gru)\n",
    "    result(all_y_pred_gru, all_y_test_gru, \"GRU\", time_to_train, time_to_predict_fold)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"GRU\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **34. Stochastic Gradient with k-fold Cross-Validation**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_sgd = []\n",
    "    all_y_pred_sgd = []\n",
    "    start_cv_sgd = time.time()\n",
    "    time_to_predict_fold = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        # Define the SGD classifier pipeline with standard scaler\n",
    "        sgd_fold = make_pipeline(StandardScaler(), SGDClassifier(random_state=24))\n",
    "        # Measure time to train on the fold\n",
    "        start_train_fold = time.time()\n",
    "        sgd_fold.fit(X_train_fold, y_train_fold)\n",
    "        end_train_fold = time.time()\n",
    "        time_to_train += end_train_fold -start_train_fold\n",
    "        # Measure time to predict on the fold\n",
    "        start_predict_fold = time.time()\n",
    "        y_pred_fold = sgd_fold.predict(X_test_fold)\n",
    "        end_predict_fold = time.time()\n",
    "        time_to_predict_fold += end_predict_fold - start_predict_fold\n",
    "        # Append metrics to lists\n",
    "        all_y_test_sgd.extend(y_test_fold)\n",
    "        all_y_pred_sgd.extend(y_pred_fold)\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Stochastic Gradient\")\n",
    "        pname = method + \"_fold_\" + str(fold_number) + \"_Stochastic_gradient_confusion_matrix.png\"\n",
    "        plt.savefig(pname)\n",
    "        #plt.show()\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_sgd = time.time()\n",
    "    result(all_y_pred_sgd, all_y_test_sgd, \"Stochastic_gradient\", time_to_train, time_to_predict_fold)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Stochastic_gradient\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **36. Extra Trees Classifier with k-fold Cross-Validation**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_extra_trees = []\n",
    "    all_y_pred_extra_trees = []\n",
    "    start_cv_extra_trees = time.time()\n",
    "    time_to_predict_fold_extra_trees = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Define the Extra Trees classifier\n",
    "        extra_trees_fold = ExtraTreesClassifier()\n",
    "\n",
    "        # Measure time to train on the fold\n",
    "        start_train_fold_extra_trees = time.time()\n",
    "        extra_trees_fold.fit(X_train_fold, y_train_fold)\n",
    "        end_train_fold_extra_trees = time.time()\n",
    "        time_to_train += end_train_fold_extra_trees -start_train_fold_extra_trees\n",
    "        # Measure time to predict on the fold\n",
    "        start_predict_fold_extra_trees = time.time()\n",
    "        y_pred_fold_extra_trees = extra_trees_fold.predict(X_test_fold)\n",
    "        end_predict_fold_extra_trees = time.time()\n",
    "\n",
    "        time_to_predict_fold_extra_trees += end_predict_fold_extra_trees - start_predict_fold_extra_trees\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_extra_trees.extend(y_test_fold)\n",
    "        all_y_pred_extra_trees.extend(y_pred_fold_extra_trees)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_extra_trees = confusion_matrix(y_test_fold, y_pred_fold_extra_trees)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_extra_trees = ConfusionMatrixDisplay(confusion_matrix=cm_extra_trees)\n",
    "        disp_extra_trees.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Extra Trees Classifier\")\n",
    "        pname_extra_trees = method + \"_fold_\" + str(fold_number) + \"_ExtraTreesClassifier_confusion_matrix.png\"\n",
    "        plt.savefig(pname_extra_trees)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_extra_trees = time.time()\n",
    "    result(all_y_pred_extra_trees, all_y_test_extra_trees, \"Extra Trees Classifier\", time_to_train, time_to_predict_fold_extra_trees)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Extra Trees Classifier\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **37. Feed Forward Neural Networks with k-fold Cross-Validation**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_ffnn = []\n",
    "    all_y_pred_ffnn = []\n",
    "    start_cv_ffnn = time.time()\n",
    "    time_to_predict_fold_ffnn = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Define the neural network architecture\n",
    "        model_ffnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_fold.shape[1],)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(len(np.unique(y_train_fold)), activation='softmax')\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model_ffnn.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        # Measure time to train on the fold\n",
    "        start_train_fold_ffnn = time.time()\n",
    "        history_ffnn = model_ffnn.fit(X_train_fold, y_train_fold, epochs=10, batch_size=50, verbose=1)\n",
    "        end_train_fold_ffnn = time.time()\n",
    "        time_to_train += end_train_fold_ffnn -start_train_fold_ffnn\n",
    "        # Measure time to predict on the fold\n",
    "        start_predict_fold_ffnn = time.time()\n",
    "        y_pred_fold_ffnn_prob = model_ffnn.predict(X_test_fold)\n",
    "        y_pred_fold_ffnn = np.argmax(y_pred_fold_ffnn_prob, axis=1)\n",
    "        end_predict_fold_ffnn = time.time()\n",
    "\n",
    "        time_to_predict_fold_ffnn += end_predict_fold_ffnn - start_predict_fold_ffnn\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_ffnn.extend(y_test_fold)\n",
    "        all_y_pred_ffnn.extend(y_pred_fold_ffnn)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_ffnn = confusion_matrix(y_test_fold, y_pred_fold_ffnn)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_ffnn = ConfusionMatrixDisplay(confusion_matrix=cm_ffnn)\n",
    "        disp_ffnn.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Feed Forward Neural Networks\")\n",
    "        pname_ffnn = method + \"_fold_\" + str(fold_number) + \"_FFNN_confusion_matrix.png\"\n",
    "        plt.savefig(pname_ffnn)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_ffnn = time.time()\n",
    "    result(all_y_pred_ffnn, all_y_test_ffnn, \"Feed Forward Neural Networks\", time_to_train, time_to_predict_fold_ffnn)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Feed Forward Neural Networks\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **38. Fuzzy with k-fold Cross-Validation**\n",
    "\n",
    "    all_y_test_fuzzy = []\n",
    "    all_y_pred_fuzzy = []\n",
    "    start_cv_fuzzy = time.time()\n",
    "    time_to_predict_fold_fuzzy = 0\n",
    "    time_to_train=0\n",
    "    # Generate fuzzy c-means clusters\n",
    "    n_clusters = 10  # Number of classes\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        start_train_fold_fuzzy = time.time()\n",
    "        # Generate fuzzy c-means clusters for training data of the fold\n",
    "        centers, u_train_fold, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "            X_train_fold.T, n_clusters, 2, error=0.005, maxiter=1000\n",
    "        )\n",
    "\n",
    "        # Measure time to train on the fold\n",
    "        end_train_fold_fuzzy = time.time()\n",
    "        time_to_train  += end_train_fold_fuzzy - start_train_fold_fuzzy\n",
    "        # Predict cluster membership for test data of the fold\n",
    "        start_predict_fold_fuzzy = time.time()\n",
    "        u_test_fold, _, _, _, _, _ = fuzz.cluster.cmeans_predict(\n",
    "            X_test_fold.T, centers, 2, error=0.005, maxiter=1000\n",
    "        )\n",
    "        end_predict_fold_fuzzy = time.time()\n",
    "\n",
    "        time_to_predict_fold_fuzzy += end_predict_fold_fuzzy - start_predict_fold_fuzzy\n",
    "\n",
    "        # Assign class labels based on cluster membership\n",
    "        y_pred_fold_fuzzy = np.argmax(u_test_fold, axis=0)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_fuzzy.extend(y_test_fold)\n",
    "        all_y_pred_fuzzy.extend(y_pred_fold_fuzzy)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_fuzzy = confusion_matrix(y_test_fold, y_pred_fold_fuzzy)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_fuzzy = ConfusionMatrixDisplay(confusion_matrix=cm_fuzzy)\n",
    "        disp_fuzzy.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Fuzzy\")\n",
    "        pname_fuzzy = method + \"_fold_\" + str(fold_number) + \"_Fuzzy_confusion_matrix.png\"\n",
    "        plt.savefig(pname_fuzzy)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_fuzzy = time.time()\n",
    "    result(all_y_pred_fuzzy, all_y_test_fuzzy, \"Fuzzy\", time_to_train, time_to_predict_fold_fuzzy)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Fuzzy\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **39. Ensemble of Deep Learning Networks (EDLNs) with k-fold Cross-Validation**\n",
    "    # Define the architecture of your neural network (example architecture)\n",
    "    def create_model(input_shape, num_classes):\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(64, activation='relu', input_shape=input_shape),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_EDLN = []\n",
    "    all_y_pred_EDLN = []\n",
    "    start_cv_EDLN = time.time()\n",
    "    time_to_predict_fold_EDLN = 0\n",
    "    time_to_train=0\n",
    "    # Define hyperparameters\n",
    "    num_networks = 5\n",
    "    epochs = 10\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_EDLN, X_test_fold_EDLN = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold_EDLN, y_test_fold_EDLN = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Train multiple neural networks\n",
    "        start = time.time()\n",
    "        models_fold_EDLN = []\n",
    "        for i in range(num_networks):\n",
    "            model_fold_EDLN = create_model(input_shape=X_train_fold_EDLN.shape[1:], num_classes=num_classes)\n",
    "            model_fold_EDLN.fit(X_train_fold_EDLN, y_train_fold_EDLN, epochs=epochs, verbose=0)\n",
    "            models_fold_EDLN.append(model_fold_EDLN)\n",
    "        end = time.time()\n",
    "        time_to_train += end - start\n",
    "        # Measure time to predict on the fold\n",
    "        start_predict_fold_EDLN = time.time()\n",
    "        # Make predictions on test data using each model\n",
    "        predictions_fold_EDLN = np.array([model_fold_EDLN.predict(X_test_fold_EDLN) for model_fold_EDLN in models_fold_EDLN])\n",
    "        end_predict_fold_EDLN = time.time()\n",
    "\n",
    "        time_to_predict_fold_EDLN += end_predict_fold_EDLN - start_predict_fold_EDLN\n",
    "\n",
    "        # Aggregate predictions by averaging\n",
    "        y_pred_fold_EDLN = np.argmax(np.mean(predictions_fold_EDLN, axis=0), axis=1)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_EDLN.extend(y_test_fold_EDLN)\n",
    "        all_y_pred_EDLN.extend(y_pred_fold_EDLN)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_EDLN = confusion_matrix(y_test_fold_EDLN, y_pred_fold_EDLN)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_EDLN = ConfusionMatrixDisplay(confusion_matrix=cm_EDLN)\n",
    "        disp_EDLN.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - EDLNs\")\n",
    "        pname_EDLN = method + \"_fold_\" + str(fold_number) + \"_EDLNs_confusion_matrix.png\"\n",
    "        plt.savefig(pname_EDLN)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_EDLN = time.time()\n",
    "    result(all_y_pred_EDLN, all_y_test_EDLN, \"EDLNs\", time_to_train, time_to_predict_fold_EDLN)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"EDLNs\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    ## **40. Gaussian Mixture Model (GMM) with k-fold Cross-Validation**\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_gmm = []\n",
    "    all_y_pred_gmm = []\n",
    "    start_cv_gmm = time.time()\n",
    "    time_to_predict_fold_gmm = 0\n",
    "    time_to_train=0\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_gmm, test_index_gmm) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_gmm, X_test_fold_gmm = X_train.iloc[train_index_gmm], X_train.iloc[test_index_gmm]\n",
    "        y_train_fold_gmm, y_test_fold_gmm = y_train.iloc[train_index_gmm], y_train.iloc[test_index_gmm]\n",
    "\n",
    "        # Number of classes\n",
    "        n_classes_gmm = len(set(y_train_fold_gmm))\n",
    "\n",
    "        # Dictionary to store GMMs for each class\n",
    "        gmm_models_fold_gmm = {}\n",
    "\n",
    "        # Train GMMs for each class\n",
    "        start_train_fold_gmm = time.time()\n",
    "        for i in range(n_classes_gmm):\n",
    "            # Filter data for the current class\n",
    "            X_class_gmm = X_train_fold_gmm[y_train_fold_gmm == i]\n",
    "            # Fit Gaussian Mixture Model\n",
    "            gmm_fold_gmm = GaussianMixture(n_components=2)  # You can adjust n_components as needed\n",
    "            gmm_fold_gmm.fit(X_class_gmm)\n",
    "            # Store the trained GMM\n",
    "            gmm_models_fold_gmm[i] = gmm_fold_gmm\n",
    "        end_train_fold_gmm = time.time()\n",
    "        time_to_train += end_train_fold_gmm - start_train_fold_gmm\n",
    "        # Measure time to predict on the fold\n",
    "        start_predict_fold_gmm = time.time()\n",
    "        y_pred_fold_gmm = []\n",
    "        for x_gmm in X_test_fold_gmm.values:  # Convert DataFrame to numpy array for iteration\n",
    "            class_likelihoods_gmm = []\n",
    "            # Reshape x to have the appropriate dimensions\n",
    "            x_reshaped_gmm = x_gmm.reshape(1, -1)\n",
    "            # Calculate likelihood for each class\n",
    "            for i in range(n_classes_gmm):\n",
    "                class_likelihood_gmm = gmm_models_fold_gmm[i].score_samples(x_reshaped_gmm)\n",
    "                class_likelihoods_gmm.append(class_likelihood_gmm)\n",
    "            # Assign the class with the highest likelihood\n",
    "            predicted_class_gmm = max(zip(class_likelihoods_gmm, range(n_classes_gmm)))[1]\n",
    "            y_pred_fold_gmm.append(predicted_class_gmm)\n",
    "        end_predict_fold_gmm = time.time()\n",
    "\n",
    "        time_to_predict_fold_gmm += end_predict_fold_gmm - start_predict_fold_gmm\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_gmm.extend(y_test_fold_gmm)\n",
    "        all_y_pred_gmm.extend(y_pred_fold_gmm)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_gmm = confusion_matrix(y_test_fold_gmm, y_pred_fold_gmm)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_gmm = ConfusionMatrixDisplay(confusion_matrix=cm_gmm)\n",
    "        disp_gmm.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - GMM\")\n",
    "        pname_gmm = method + \"_fold_\" + str(fold_number) + \"_GMM_confusion_matrix.png\"\n",
    "        plt.savefig(pname_gmm)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_gmm = time.time()\n",
    "    result(all_y_pred_gmm, all_y_test_gmm, \"GMM\", time_to_train, time_to_predict_fold_gmm)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"GMM Completed :)  \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"GMM\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## **41. Bernoulli Naive Bayes with k-fold Cross-Validation**\"\"\"\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_bnb = []\n",
    "    all_y_pred_bnb = []\n",
    "    start_cv_bnb = time.time()\n",
    "    time_to_predict_fold_bnb = 0\n",
    "    time_to_train=0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_bnb, test_index_bnb) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_bnb, X_test_fold_bnb = X_train.iloc[train_index_bnb], X_train.iloc[test_index_bnb]\n",
    "        y_train_fold_bnb, y_test_fold_bnb = y_train.iloc[train_index_bnb], y_train.iloc[test_index_bnb]\n",
    "\n",
    "        # Create a Bernoulli Naive Bayes classifier\n",
    "        bnb_fold_bnb = BernoulliNB()\n",
    "\n",
    "        # Train the classifier\n",
    "        start_train_fold_bnb = time.time()\n",
    "        bnb_fold_bnb.fit(X_train_fold_bnb, y_train_fold_bnb)\n",
    "        end_train_fold_bnb = time.time()\n",
    "        time_to_train += end_train_fold_bnb - start_train_fold_bnb\n",
    "\n",
    "        # Predict using the trained model\n",
    "        start_predict_fold_bnb = time.time()\n",
    "        y_pred_fold_bnb = bnb_fold_bnb.predict(X_test_fold_bnb)\n",
    "        end_predict_fold_bnb = time.time()\n",
    "        time_to_predict_fold_bnb += end_predict_fold_bnb - start_predict_fold_bnb\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_bnb.extend(y_test_fold_bnb)\n",
    "        all_y_pred_bnb.extend(y_pred_fold_bnb)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_bnb = confusion_matrix(y_test_fold_bnb, y_pred_fold_bnb)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_bnb = ConfusionMatrixDisplay(confusion_matrix=cm_bnb)\n",
    "        disp_bnb.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Bernoulli Naive Bayes\")\n",
    "        pname_bnb = method + \"_fold_\" + str(fold_number) + \"_Bernoulli_Naive_Bayes_confusion_matrix.png\"\n",
    "        plt.savefig(pname_bnb)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_bnb = time.time()\n",
    "    result(all_y_pred_bnb, all_y_test_bnb, \"Bernoulli Naive Bayes\", time_to_train, time_to_predict_fold_bnb)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Bernoulli Naive Bayes with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Bernoulli Naive Bayes\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## **42. CatBoost with k-fold Cross-Validation**\"\"\"\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_catboost = []\n",
    "    all_y_pred_catboost = []\n",
    "    start_cv_catboost = time.time()\n",
    "    time_to_predict_fold_catboost = 0\n",
    "    time_to_train=0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_catboost, test_index_catboost) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_catboost, X_test_fold_catboost = X_train.iloc[train_index_catboost], X_train.iloc[test_index_catboost]\n",
    "        y_train_fold_catboost, y_test_fold_catboost = y_train.iloc[train_index_catboost], y_train.iloc[test_index_catboost]\n",
    "\n",
    "        # Create and train the CatBoost model\n",
    "        start_train_fold_catboost = time.time()\n",
    "        catboost_model_fold_catboost = CatBoostClassifier(random_state=42)\n",
    "        catboost_model_fold_catboost.fit(X_train_fold_catboost, y_train_fold_catboost)\n",
    "        end_train_fold_catboost = time.time()\n",
    "        time_to_train += end_train_fold_catboost - start_train_fold_catboost\n",
    "\n",
    "        # Predict using the trained model\n",
    "        start_predict_fold_catboost = time.time()\n",
    "        y_pred_fold_catboost = catboost_model_fold_catboost.predict(X_test_fold_catboost)\n",
    "        end_predict_fold_catboost = time.time()\n",
    "        time_to_predict_fold_catboost += end_predict_fold_catboost - start_predict_fold_catboost\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_catboost.extend(y_test_fold_catboost)\n",
    "        all_y_pred_catboost.extend(y_pred_fold_catboost)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_catboost = confusion_matrix(y_test_fold_catboost, y_pred_fold_catboost)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_catboost = ConfusionMatrixDisplay(confusion_matrix=cm_catboost)\n",
    "        disp_catboost.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - CatBoost\")\n",
    "        pname_catboost = method + \"_fold_\" + str(fold_number) + \"_CatBoost_confusion_matrix.png\"\n",
    "        plt.savefig(pname_catboost)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_catboost = time.time()\n",
    "    result(all_y_pred_catboost, all_y_test_catboost, \"CatBoost\", time_to_train, time_to_predict_fold_catboost)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"CatBoost with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"CatBoost\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## **43. Centralised blending with k-fold Cross-Validation**\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_blend = []\n",
    "    all_y_pred_blend = []\n",
    "    start_cv_blend = time.time()\n",
    "    time_to_predict_fold_blend = 0\n",
    "    time_to_train = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_blend, test_index_blend) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_blend, X_test_fold_blend = X_train.iloc[train_index_blend], X_train.iloc[test_index_blend]\n",
    "        y_train_fold_blend, y_test_fold_blend = y_train.iloc[train_index_blend], y_train.iloc[test_index_blend]\n",
    "\n",
    "        # Define base models\n",
    "        base_model1 = DecisionTreeClassifier(random_state=24)\n",
    "        base_model2 = RandomForestClassifier(random_state=24)\n",
    "        base_model3 = LogisticRegression(random_state=24)\n",
    "\n",
    "        # Train base models\n",
    "        start_train_fold_blend = time.time()\n",
    "        base_model1.fit(X_train_fold_blend, y_train_fold_blend)\n",
    "        base_model2.fit(X_train_fold_blend, y_train_fold_blend)\n",
    "        base_model3.fit(X_train_fold_blend, y_train_fold_blend)\n",
    "        end_train_fold_blend = time.time()\n",
    "        time_to_train += end_train_fold_blend - start_train_fold_blend\n",
    "\n",
    "        # Make predictions on validation data\n",
    "        preds_val_base_model1 = base_model1.predict(X_test_fold_blend)\n",
    "        preds_val_base_model2 = base_model2.predict(X_test_fold_blend)\n",
    "        preds_val_base_model3 = base_model3.predict(X_test_fold_blend)\n",
    "\n",
    "        # Combine predictions from base models into a feature matrix for meta-model\n",
    "        X_val_meta_blend = np.column_stack((preds_val_base_model1, preds_val_base_model2, preds_val_base_model3))\n",
    "\n",
    "        # Train meta-model (blender)\n",
    "        blender_blend = LogisticRegression(random_state=24)\n",
    "        blender_blend.fit(X_val_meta_blend, y_test_fold_blend)\n",
    "\n",
    "        # Make predictions on test data using base models\n",
    "        preds_test_base_model1 = base_model1.predict(X_test_fold_blend)\n",
    "        preds_test_base_model2 = base_model2.predict(X_test_fold_blend)\n",
    "        preds_test_base_model3 = base_model3.predict(X_test_fold_blend)\n",
    "\n",
    "        # Combine predictions from base models into a feature matrix for meta-model\n",
    "        X_test_meta_blend = np.column_stack((preds_test_base_model1, preds_test_base_model2, preds_test_base_model3))\n",
    "\n",
    "        # Make predictions on test data using meta-model\n",
    "        preds_test_meta_blend = blender_blend.predict(X_test_meta_blend)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_blend.extend(y_test_fold_blend)\n",
    "        all_y_pred_blend.extend(preds_test_meta_blend)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_blend = confusion_matrix(y_test_fold_blend, preds_test_meta_blend)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_blend = ConfusionMatrixDisplay(confusion_matrix=cm_blend)\n",
    "        disp_blend.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Centralised Blending\")\n",
    "        pname_blend = method + \"_fold_\" + str(fold_number) + \"_Centralised_Blending_confusion_matrix.png\"\n",
    "        plt.savefig(pname_blend)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_blend = time.time()\n",
    "    result(all_y_pred_blend, all_y_test_blend, \"Centralised Blending\", time_to_train, time_to_predict_fold_blend)\n",
    "\n",
    "    \"\"\"44.## Binary Logical Circular Neural Network (BLoCNet) with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_blocnet = []\n",
    "    all_y_pred_blocnet = []\n",
    "    start_cv_blocnet = time.time()\n",
    "    time_to_predict_fold_blocnet = 0\n",
    "    time_to_train = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_blocnet, test_index_blocnet) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_blocnet, X_test_fold_blocnet = X_train.iloc[train_index_blocnet], X_train.iloc[test_index_blocnet]\n",
    "        y_train_fold_blocnet, y_test_fold_blocnet = y_train.iloc[train_index_blocnet], y_train.iloc[test_index_blocnet]\n",
    "\n",
    "        # Define the architecture of the BLoCNet\n",
    "        model_blocnet = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_fold_blocnet.shape[1],)),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(9, activation='softmax')  # Multi-class classification, so softmax activation\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model_blocnet.compile(optimizer='adam',\n",
    "                            loss='sparse_categorical_crossentropy',  # Multi-class classification, so sparse categorical crossentropy loss\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        start_train_fold_blocnet = time.time()\n",
    "        history_blocnet = model_blocnet.fit(X_train_fold_blocnet, y_train_fold_blocnet, epochs=10, batch_size=32, validation_split=0.2)\n",
    "        end_train_fold_blocnet = time.time()\n",
    "        time_to_train += end_train_fold_blocnet - start_train_fold_blocnet\n",
    "\n",
    "        # Evaluate the model\n",
    "        start_predict_fold_blocnet = time.time()\n",
    "        y_pred_fold_blocnet = model_blocnet.predict(X_test_fold_blocnet)\n",
    "        y_pred_fold_blocnet = np.argmax(y_pred_fold_blocnet, axis=1)\n",
    "        end_predict_fold_blocnet = time.time()\n",
    "        time_to_predict_fold_blocnet += end_predict_fold_blocnet - start_predict_fold_blocnet\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_blocnet.extend(y_test_fold_blocnet)\n",
    "        all_y_pred_blocnet.extend(y_pred_fold_blocnet)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_blocnet = confusion_matrix(y_test_fold_blocnet, y_pred_fold_blocnet)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_blocnet = ConfusionMatrixDisplay(confusion_matrix=cm_blocnet)\n",
    "        disp_blocnet.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - BLoCNet\")\n",
    "        pname_blocnet = method + \"_fold_\" + str(fold_number) + \"_BLoCNet_confusion_matrix.png\"\n",
    "        plt.savefig(pname_blocnet)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_blocnet = time.time()\n",
    "    result(all_y_pred_blocnet, all_y_test_blocnet, \"BLoCNet\", time_to_train, time_to_predict_fold_blocnet)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"BLoCNet with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"BLoCNet\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 45.constructive_learning with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_constructive_learning = []\n",
    "    all_y_pred_constructive_learning = []\n",
    "    start_cv_constructive_learning = time.time()\n",
    "    time_to_predict_fold_constructive_learning = 0\n",
    "    time_to_train = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_constructive_learning, test_index_constructive_learning) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_constructive_learning, X_test_fold_constructive_learning = X_train.iloc[train_index_constructive_learning], X_train.iloc[test_index_constructive_learning]\n",
    "        y_train_fold_constructive_learning, y_test_fold_constructive_learning = y_train.iloc[train_index_constructive_learning], y_train.iloc[test_index_constructive_learning]\n",
    "\n",
    "        # Create a basic Decision Tree model for this fold\n",
    "        base_model_fold_constructive_learning = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "        # Train the base model for this fold\n",
    "        start_train_fold_constructive_learning = time.time()\n",
    "        base_model_fold_constructive_learning.fit(X_train_fold_constructive_learning, y_train_fold_constructive_learning)\n",
    "        end_train_fold_constructive_learning = time.time()\n",
    "        time_to_train += end_train_fold_constructive_learning - start_train_fold_constructive_learning\n",
    "\n",
    "        # Evaluate the base model for this fold\n",
    "        start_predict_fold_constructive_learning = time.time()\n",
    "        y_pred_fold_constructive_learning = base_model_fold_constructive_learning.predict(X_test_fold_constructive_learning)\n",
    "        end_predict_fold_constructive_learning = time.time()\n",
    "        time_to_predict_fold_constructive_learning += end_predict_fold_constructive_learning - start_predict_fold_constructive_learning\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_constructive_learning.extend(y_test_fold_constructive_learning)\n",
    "        all_y_pred_constructive_learning.extend(y_pred_fold_constructive_learning)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_constructive_learning = confusion_matrix(y_test_fold_constructive_learning, y_pred_fold_constructive_learning)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_constructive_learning = ConfusionMatrixDisplay(confusion_matrix=cm_constructive_learning)\n",
    "        disp_constructive_learning.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - constructive_learning\")\n",
    "        pname_constructive_learning = method + \"_fold_\" + str(fold_number) + \"_constructive_learning_confusion_matrix.png\"\n",
    "        plt.savefig(pname_constructive_learning)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_constructive_learning = time.time()\n",
    "    result(all_y_pred_constructive_learning, all_y_test_constructive_learning, \"constructive_learning\", time_to_train, time_to_predict_fold_constructive_learning)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"constructive_learning with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"constructive_learning\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 46. Artificial Immune System (AIS) with k-fold Cross-Validation\"\"\"\n",
    "    class AISModel:\n",
    "        def __init__(self, base_model):\n",
    "            self.base_model = base_model\n",
    "\n",
    "        def fit(self, X_train, y_train):\n",
    "            # AIS training algorithm\n",
    "            self.base_model.fit(X_train, y_train)\n",
    "\n",
    "        def predict(self, X_test):\n",
    "            # AIS prediction algorithm\n",
    "            y_pred = self.base_model.predict(X_test)\n",
    "            return y_pred\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_ais = []\n",
    "    all_y_pred_ais = []\n",
    "    start_cv_ais = time.time()\n",
    "    time_to_predict_fold_ais = 0\n",
    "    time_to_train = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_ais, test_index_ais) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_ais, X_test_fold_ais = X_train.iloc[train_index_ais], X_train.iloc[test_index_ais]\n",
    "        y_train_fold_ais, y_test_fold_ais = y_train.iloc[train_index_ais], y_train.iloc[test_index_ais]\n",
    "\n",
    "        # Create an instance of RandomForestClassifier as the base learner for this fold\n",
    "        base_model_fold_ais = RandomForestClassifier(random_state=24)\n",
    "\n",
    "        # Create an instance of AISModel with the base learner for this fold\n",
    "        ais_model_fold_ais = AISModel(base_model_fold_ais)\n",
    "\n",
    "        # Train the AIS model for this fold\n",
    "        start_train_fold_ais = time.time()\n",
    "        ais_model_fold_ais.fit(X_train_fold_ais, y_train_fold_ais)\n",
    "        end_train_fold_ais = time.time()\n",
    "        time_to_train += end_train_fold_ais - start_train_fold_ais\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_ais = time.time()\n",
    "        y_pred_fold_ais = ais_model_fold_ais.predict(X_test_fold_ais)\n",
    "        end_predict_fold_ais = time.time()\n",
    "        time_to_predict_fold_ais += end_predict_fold_ais - start_predict_fold_ais\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_ais.extend(y_test_fold_ais)\n",
    "        all_y_pred_ais.extend(y_pred_fold_ais)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_ais = confusion_matrix(y_test_fold_ais, y_pred_fold_ais)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_ais = ConfusionMatrixDisplay(confusion_matrix=cm_ais)\n",
    "        disp_ais.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - AIS\")\n",
    "        pname_ais = method + \"_fold_\" + str(fold_number) + \"_AIS_confusion_matrix.png\"\n",
    "        plt.savefig(pname_ais)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_ais = time.time()\n",
    "    result(all_y_pred_ais, all_y_test_ais, \"AIS\", time_to_train, time_to_predict_fold_ais)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"AIS with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"AIS\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 48. GBBK Algorithm with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_gbkk = []\n",
    "    all_y_pred_gbkk = []\n",
    "    start_cv_gbkk = time.time()\n",
    "    time_to_predict_fold_gbkk = 0\n",
    "    time_to_train_gbkk = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_gbkk, test_index_gbkk) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_gbkk, X_test_fold_gbkk = X_train.iloc[train_index_gbkk], X_train.iloc[test_index_gbkk]\n",
    "        y_train_fold_gbkk, y_test_fold_gbkk = y_train.iloc[train_index_gbkk], y_train.iloc[test_index_gbkk]\n",
    "\n",
    "        # Initialize Gaussian Naive Bayes (GBBK) model for this fold\n",
    "        gbbk_model_fold_gbkk = GaussianNB()\n",
    "\n",
    "        # Train the GBBK model for this fold\n",
    "        start_train_fold_gbkk = time.time()\n",
    "        gbbk_model_fold_gbkk.fit(X_train_fold_gbkk, y_train_fold_gbkk)\n",
    "        end_train_fold_gbkk = time.time()\n",
    "        time_to_train_gbkk += end_train_fold_gbkk - start_train_fold_gbkk\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_gbkk = time.time()\n",
    "        y_pred_fold_gbkk = gbbk_model_fold_gbkk.predict(X_test_fold_gbkk)\n",
    "        end_predict_fold_gbkk = time.time()\n",
    "        time_to_predict_fold_gbkk += end_predict_fold_gbkk - start_predict_fold_gbkk\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_gbkk.extend(y_test_fold_gbkk)\n",
    "        all_y_pred_gbkk.extend(y_pred_fold_gbkk)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_gbkk = confusion_matrix(y_test_fold_gbkk, y_pred_fold_gbkk)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_gbkk = ConfusionMatrixDisplay(confusion_matrix=cm_gbkk)\n",
    "        disp_gbkk.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - GBBK\")\n",
    "        pname_gbkk = method + \"_fold_\" + str(fold_number) + \"_GBBK_confusion_matrix.png\"\n",
    "        plt.savefig(pname_gbkk)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_gbkk = time.time()\n",
    "    result(all_y_pred_gbkk, all_y_test_gbkk, \"GBBK\", time_to_train_gbkk, time_to_predict_fold_gbkk)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"GBBK with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"GBBK\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 48. GE SVM Algorithm with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_gbkk = []\n",
    "    all_y_pred_gbkk = []\n",
    "    start_cv_gbkk = time.time()\n",
    "    time_to_predict_fold_gbkk = 0\n",
    "    time_to_train_gbkk = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_gbkk, test_index_gbkk) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_gbkk, X_test_fold_gbkk = X_train.iloc[train_index_gbkk], X_train.iloc[test_index_gbkk]\n",
    "        y_train_fold_gbkk, y_test_fold_gbkk = y_train.iloc[train_index_gbkk], y_train.iloc[test_index_gbkk]\n",
    "\n",
    "        # Initialize Gaussian Naive Bayes (GBBK) model for this fold\n",
    "        gbbk_model_fold_gbkk = SVC(kernel='rbf', decision_function_shape='ovo')\n",
    "\n",
    "        # Train the GBBK model for this fold\n",
    "        start_train_fold_gbkk = time.time()\n",
    "        gbbk_model_fold_gbkk.fit(X_train_fold_gbkk, y_train_fold_gbkk)\n",
    "        end_train_fold_gbkk = time.time()\n",
    "        time_to_train_gbkk += end_train_fold_gbkk - start_train_fold_gbkk\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_gbkk = time.time()\n",
    "        y_pred_fold_gbkk = gbbk_model_fold_gbkk.predict(X_test_fold_gbkk)\n",
    "        end_predict_fold_gbkk = time.time()\n",
    "        time_to_predict_fold_gbkk += end_predict_fold_gbkk - start_predict_fold_gbkk\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_gbkk.extend(y_test_fold_gbkk)\n",
    "        all_y_pred_gbkk.extend(y_pred_fold_gbkk)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_gbkk = confusion_matrix(y_test_fold_gbkk, y_pred_fold_gbkk)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_gbkk = ConfusionMatrixDisplay(confusion_matrix=cm_gbkk)\n",
    "        disp_gbkk.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - GE SVM\")\n",
    "        pname_gbkk = method + \"_fold_\" + str(fold_number) + \"_GE_SVM_confusion_matrix.png\"\n",
    "        plt.savefig(pname_gbkk)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_gbkk = time.time()\n",
    "    result(all_y_pred_gbkk, all_y_test_gbkk, \"GE SVM\", time_to_train_gbkk, time_to_predict_fold_gbkk)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"GE SVM with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"GE SVM\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 51. Hidden Naive Bayes (HNB) with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_hnb = []\n",
    "    all_y_pred_hnb = []\n",
    "    start_cv_hnb = time.time()\n",
    "    time_to_predict_fold_hnb = 0\n",
    "    time_to_train_hnb = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_hnb, test_index_hnb) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_hnb, X_test_fold_hnb = X_train.iloc[train_index_hnb], X_train.iloc[test_index_hnb]\n",
    "        y_train_fold_hnb, y_test_fold_hnb = y_train.iloc[train_index_hnb], y_train.iloc[test_index_hnb]\n",
    "\n",
    "        # Train the HNB model for this fold\n",
    "        hnb_model_fold = GaussianNB()\n",
    "        start_train_fold_hnb = time.time()\n",
    "        hnb_model_fold.fit(pd.DataFrame(X_train_fold_hnb), pd.DataFrame(y_train_fold_hnb))\n",
    "        end_train_fold_hnb = time.time()\n",
    "        time_to_train_hnb += end_train_fold_hnb - start_train_fold_hnb\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_hnb = time.time()\n",
    "        y_pred_fold_hnb = hnb_model_fold.predict(X_test_fold_hnb.values)\n",
    "        end_predict_fold_hnb = time.time()\n",
    "        time_to_predict_fold_hnb += end_predict_fold_hnb - start_predict_fold_hnb\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_hnb.extend(y_test_fold_hnb)\n",
    "        all_y_pred_hnb.extend(y_pred_fold_hnb)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_hnb = confusion_matrix(y_test_fold_hnb, y_pred_fold_hnb)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_hnb = ConfusionMatrixDisplay(confusion_matrix=cm_hnb)\n",
    "        disp_hnb.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - HNB\")\n",
    "        pname_hnb = method + \"_fold_\" + str(fold_number) + \"_HNB_confusion_matrix.png\"\n",
    "        plt.savefig(pname_hnb)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_hnb = time.time()\n",
    "    result(all_y_pred_hnb, all_y_test_hnb, \"Hidden Naive Bayes (HNB)\", time_to_train_hnb, time_to_predict_fold_hnb)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"HNB with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Hidden Naive Bayes\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 52. HistGradientBoostingClassifier with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_hgb = []\n",
    "    all_y_pred_hgb = []\n",
    "    start_cv_hgb = time.time()\n",
    "    time_to_predict_fold_hgb = 0\n",
    "    time_to_train_hgb = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_hgb, test_index_hgb) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_hgb, X_test_fold_hgb = X_train.iloc[train_index_hgb], X_train.iloc[test_index_hgb]\n",
    "        y_train_fold_hgb, y_test_fold_hgb = y_train.iloc[train_index_hgb], y_train.iloc[test_index_hgb]\n",
    "\n",
    "        # Train the HistGradientBoostingClassifier model for this fold\n",
    "        hgb_model_fold = HistGradientBoostingClassifier(random_state=24)\n",
    "        start_train_fold_hgb = time.time()\n",
    "        hgb_model_fold.fit(X_train_fold_hgb, y_train_fold_hgb)\n",
    "        end_train_fold_hgb = time.time()\n",
    "        time_to_train_hgb += end_train_fold_hgb - start_train_fold_hgb\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_hgb = time.time()\n",
    "        y_pred_fold_hgb = hgb_model_fold.predict(X_test_fold_hgb)\n",
    "        end_predict_fold_hgb = time.time()\n",
    "        time_to_predict_fold_hgb += end_predict_fold_hgb - start_predict_fold_hgb\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_hgb.extend(y_test_fold_hgb)\n",
    "        all_y_pred_hgb.extend(y_pred_fold_hgb)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_hgb = confusion_matrix(y_test_fold_hgb, y_pred_fold_hgb)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_hgb = ConfusionMatrixDisplay(confusion_matrix=cm_hgb)\n",
    "        disp_hgb.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - HistGradientBoostingClassifier\")\n",
    "        pname_hgb = method + \"_fold_\" + str(fold_number) + \"_HistGradientBoostingClassifier_confusion_matrix.png\"\n",
    "        plt.savefig(pname_hgb)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_hgb = time.time()\n",
    "    result(all_y_pred_hgb, all_y_test_hgb, \"HistGradientBoostingClassifier\", time_to_train_hgb, time_to_predict_fold_hgb)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"HistGradientBoostingClassifier with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"HistGradientBoostingClassifier\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## IGRF-RFE with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize Decision Tree Classifier\n",
    "    dt_classifier = DecisionTreeClassifier(random_state=24)\n",
    "\n",
    "    # Initialize RFE (Recursive Feature Elimination) with Decision Tree Classifier as estimator\n",
    "    rfe = RFE(estimator=dt_classifier)\n",
    "\n",
    "    # Initialize k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits= n_splits_for_cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_igrf_rfe = []\n",
    "    all_y_pred_igrf_rfe = []\n",
    "    start_cv_igrf_rfe = time.time()\n",
    "    time_to_predict_fold_igrf_rfe = 0\n",
    "    time_to_train_igrf_rfe = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_igrf_rfe, test_index_igrf_rfe) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_igrf_rfe, X_test_fold_igrf_rfe = X_train.iloc[train_index_igrf_rfe], X_train.iloc[test_index_igrf_rfe]\n",
    "        y_train_fold_igrf_rfe, y_test_fold_igrf_rfe = y_train.iloc[train_index_igrf_rfe], y_train.iloc[test_index_igrf_rfe]\n",
    "\n",
    "        # Fit RFE on training data for this fold\n",
    "        start_feature_selection_fold_igrf_rfe = time.time()\n",
    "        rfe.fit(X_train_fold_igrf_rfe, y_train_fold_igrf_rfe)\n",
    "        end_feature_selection_fold_igrf_rfe = time.time()\n",
    "\n",
    "        # Select features based on RFE ranking\n",
    "        X_train_rfe_fold_igrf_rfe = rfe.transform(X_train_fold_igrf_rfe)\n",
    "        X_test_rfe_fold_igrf_rfe = rfe.transform(X_test_fold_igrf_rfe)\n",
    "\n",
    "        # Train Decision Tree Classifier using selected features for this fold\n",
    "        start_train_fold_igrf_rfe = time.time()\n",
    "        dt_classifier.fit(X_train_rfe_fold_igrf_rfe, y_train_fold_igrf_rfe)\n",
    "        end_train_fold_igrf_rfe = time.time()\n",
    "        time_to_train_igrf_rfe += end_train_fold_igrf_rfe - start_train_fold_igrf_rfe\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_igrf_rfe = time.time()\n",
    "        y_pred_fold_igrf_rfe = dt_classifier.predict(X_test_rfe_fold_igrf_rfe)\n",
    "        end_predict_fold_igrf_rfe = time.time()\n",
    "        time_to_predict_fold_igrf_rfe += end_predict_fold_igrf_rfe - start_predict_fold_igrf_rfe\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_igrf_rfe.extend(y_test_fold_igrf_rfe)\n",
    "        all_y_pred_igrf_rfe.extend(y_pred_fold_igrf_rfe)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_igrf_rfe = confusion_matrix(y_test_fold_igrf_rfe, y_pred_fold_igrf_rfe)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_igrf_rfe = ConfusionMatrixDisplay(confusion_matrix=cm_igrf_rfe)\n",
    "        disp_igrf_rfe.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - IGRF-RFE\")\n",
    "        pname_igrf_rfe = method + \"_fold_\" + str(fold_number) + \"_IGRF_RFE_confusion_matrix.png\"\n",
    "        plt.savefig(pname_igrf_rfe)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_igrf_rfe = time.time()\n",
    "    result(all_y_pred_igrf_rfe, all_y_test_igrf_rfe, \"IGRF-RFE with k-fold Cross-Validation\", time_to_train_igrf_rfe, time_to_predict_fold_igrf_rfe)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"IGRF-RFE with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"IGRF-RFE\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 55. Independent Component Analysis (ICA) with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_ica = []\n",
    "    all_y_pred_ica = []\n",
    "    start_cv_ica = time.time()\n",
    "    time_to_predict_fold_ica = 0\n",
    "    time_to_train_ica = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_ica, test_index_ica) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_ica, X_test_fold_ica = X_train.iloc[train_index_ica], X_train.iloc[test_index_ica]\n",
    "        y_train_fold_ica, y_test_fold_ica = y_train.iloc[train_index_ica], y_train.iloc[test_index_ica]\n",
    "\n",
    "        # Perform Independent Component Analysis (ICA) for dimensionality reduction\n",
    "        ica_fold = FastICA(n_components=10, random_state=42)\n",
    "        X_train_ica_fold = ica_fold.fit_transform(X_train_fold_ica)\n",
    "        X_test_ica_fold = ica_fold.transform(X_test_fold_ica)\n",
    "\n",
    "        # Normalize the data\n",
    "        scaler_fold = StandardScaler()\n",
    "        X_train_ica_fold = scaler_fold.fit_transform(X_train_ica_fold)\n",
    "        X_test_ica_fold = scaler_fold.transform(X_test_ica_fold)\n",
    "\n",
    "        # Train a RandomForestClassifier on the transformed data for this fold\n",
    "        rf_model_fold_ica = RandomForestClassifier(random_state=24)\n",
    "        start_train_fold_ica = time.time()\n",
    "        rf_model_fold_ica.fit(X_train_ica_fold, y_train_fold_ica)\n",
    "        end_train_fold_ica = time.time()\n",
    "        time_to_train_ica += end_train_fold_ica - start_train_fold_ica\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_ica = time.time()\n",
    "        y_pred_fold_ica = rf_model_fold_ica.predict(X_test_ica_fold)\n",
    "        end_predict_fold_ica = time.time()\n",
    "        time_to_predict_fold_ica += end_predict_fold_ica - start_predict_fold_ica\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_ica.extend(y_test_fold_ica)\n",
    "        all_y_pred_ica.extend(y_pred_fold_ica)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_ica = confusion_matrix(y_test_fold_ica, y_pred_fold_ica)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_ica = ConfusionMatrixDisplay(confusion_matrix=cm_ica)\n",
    "        disp_ica.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - ICA\")\n",
    "        pname_ica = method + \"_fold_\" + str(fold_number) + \"_ICA_confusion_matrix.png\"\n",
    "        plt.savefig(pname_ica)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_ica = time.time()\n",
    "    result(all_y_pred_ica, all_y_test_ica, \"ICA\", time_to_train_ica, time_to_predict_fold_ica)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"ICA with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"ICA\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 56. Lasso Regression with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_lasso = []\n",
    "    all_y_pred_lasso = []\n",
    "    start_cv_lasso = time.time()\n",
    "    time_to_predict_fold_lasso = 0\n",
    "    time_to_train_lasso = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_lasso, test_index_lasso) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_lasso, X_test_fold_lasso = X_train.iloc[train_index_lasso], X_train.iloc[test_index_lasso]\n",
    "        y_train_fold_lasso, y_test_fold_lasso = y_train.iloc[train_index_lasso], y_train.iloc[test_index_lasso]\n",
    "\n",
    "        # Initialize Lasso Regression model\n",
    "        lasso_model_fold = LogisticRegression(penalty='l1', solver='saga', random_state=24, max_iter=1000)\n",
    "\n",
    "        # Train the Lasso Regression model for this fold\n",
    "        start_train_fold_lasso = time.time()\n",
    "        lasso_model_fold.fit(X_train_fold_lasso, y_train_fold_lasso)\n",
    "        end_train_fold_lasso = time.time()\n",
    "        time_to_train_lasso += end_train_fold_lasso - start_train_fold_lasso\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_lasso = time.time()\n",
    "        y_pred_fold_lasso = lasso_model_fold.predict(X_test_fold_lasso)\n",
    "        end_predict_fold_lasso = time.time()\n",
    "        time_to_predict_fold_lasso += end_predict_fold_lasso - start_predict_fold_lasso\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_lasso.extend(y_test_fold_lasso)\n",
    "        all_y_pred_lasso.extend(y_pred_fold_lasso)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_lasso = confusion_matrix(y_test_fold_lasso, y_pred_fold_lasso)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_lasso = ConfusionMatrixDisplay(confusion_matrix=cm_lasso)\n",
    "        disp_lasso.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Lasso Regression\")\n",
    "        pname_lasso = method + \"_fold_\" + str(fold_number) + \"_Lasso_Regression_confusion_matrix.png\"\n",
    "        plt.savefig(pname_lasso)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_lasso = time.time()\n",
    "    result(all_y_pred_lasso, all_y_test_lasso, \"Lasso Regression\", time_to_train_lasso, time_to_predict_fold_lasso)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Lasso Regression with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Lasso\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 57. Meta (KNN) with Neighbors (K) and k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_meta_knn = []\n",
    "    all_y_pred_meta_knn = []\n",
    "    start_cv_meta_knn = time.time()\n",
    "    time_to_predict_fold_meta_knn = 0\n",
    "    time_to_train_meta_knn = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_meta_knn, test_index_meta_knn) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_meta_knn, X_test_fold_meta_knn = X_train.iloc[train_index_meta_knn], X_train.iloc[test_index_meta_knn]\n",
    "        y_train_fold_meta_knn, y_test_fold_meta_knn = y_train.iloc[train_index_meta_knn], y_train.iloc[test_index_meta_knn]\n",
    "\n",
    "        # Initialize base classifier (KNN) for meta-learning\n",
    "        k = 5\n",
    "        base_classifier_meta_knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "        # Initialize BaggingClassifier for meta-learning with KNN base classifier\n",
    "        meta_knn_model_fold = BaggingClassifier(base_classifier_meta_knn, n_estimators=10, random_state=42)\n",
    "\n",
    "        # Train the meta KNN model for this fold\n",
    "        start_train_fold_meta_knn = time.time()\n",
    "        meta_knn_model_fold.fit(X_train_fold_meta_knn, y_train_fold_meta_knn)\n",
    "        end_train_fold_meta_knn = time.time()\n",
    "        time_to_train_meta_knn += end_train_fold_meta_knn - start_train_fold_meta_knn\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_meta_knn = time.time()\n",
    "        y_pred_fold_meta_knn = meta_knn_model_fold.predict(X_test_fold_meta_knn)\n",
    "        end_predict_fold_meta_knn = time.time()\n",
    "        time_to_predict_fold_meta_knn += end_predict_fold_meta_knn - start_predict_fold_meta_knn\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_meta_knn.extend(y_test_fold_meta_knn)\n",
    "        all_y_pred_meta_knn.extend(y_pred_fold_meta_knn)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_meta_knn = confusion_matrix(y_test_fold_meta_knn, y_pred_fold_meta_knn)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_meta_knn = ConfusionMatrixDisplay(confusion_matrix=cm_meta_knn)\n",
    "        disp_meta_knn.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Meta (KNN)\")\n",
    "        pname_meta_knn = method + \"_fold_\" + str(fold_number) + \"_Meta_KNN_confusion_matrix.png\"\n",
    "        plt.savefig(pname_meta_knn)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_meta_knn = time.time()\n",
    "    result(all_y_pred_meta_knn, all_y_test_meta_knn, \"Meta (KNN)\", time_to_train_meta_knn, time_to_predict_fold_meta_knn)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Meta (KNN) with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"KNN\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## T-SNE Random Forest (T-SNERF) with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize T-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "    # Initialize Random Forest Classifier\n",
    "    rf_model = RandomForestClassifier(random_state=24)\n",
    "\n",
    "    # Initialize k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits= n_splits_for_cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_tsnerf = []\n",
    "    all_y_pred_tsnerf = []\n",
    "    start_cv_tsnerf = time.time()\n",
    "    time_to_predict_fold_tsnerf = 0\n",
    "    time_to_train_tsnerf = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_tsnerf, test_index_tsnerf) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_tsnerf, X_test_fold_tsnerf = X_train.iloc[train_index_tsnerf], X_train.iloc[test_index_tsnerf]\n",
    "        y_train_fold_tsnerf, y_test_fold_tsnerf = y_train.iloc[train_index_tsnerf], y_train.iloc[test_index_tsnerf]\n",
    "\n",
    "        # Train T-SNE on the training data for this fold\n",
    "        start_train_fold_tsnerf = time.time()\n",
    "        X_train_tsne_fold_tsnerf = tsne.fit_transform(X_train_fold_tsnerf)\n",
    "        end_train_fold_tsnerf = time.time()\n",
    "        time_to_train_tsnerf += end_train_fold_tsnerf - start_train_fold_tsnerf\n",
    "\n",
    "        # Train Random Forest on the T-SNE transformed data for this fold\n",
    "        start_rf_fold_tsnerf = time.time()\n",
    "        rf_model.fit(X_train_tsne_fold_tsnerf, y_train_fold_tsnerf)\n",
    "        end_rf_fold_tsnerf = time.time()\n",
    "\n",
    "        # Transform the test data using the trained T-SNE model for this fold\n",
    "        start_transform_fold_tsnerf = time.time()\n",
    "        X_test_tsne_fold_tsnerf = tsne.fit_transform(X_test_fold_tsnerf)\n",
    "        end_transform_fold_tsnerf = time.time()\n",
    "\n",
    "        # Predict using the trained Random Forest model for this fold\n",
    "        start_predict_fold_tsnerf = time.time()\n",
    "        y_pred_fold_tsnerf = rf_model.predict(X_test_tsne_fold_tsnerf)\n",
    "        end_predict_fold_tsnerf = time.time()\n",
    "        time_to_predict_fold_tsnerf += end_predict_fold_tsnerf - start_predict_fold_tsnerf\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_tsnerf.extend(y_test_fold_tsnerf)\n",
    "        all_y_pred_tsnerf.extend(y_pred_fold_tsnerf)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_tsnerf = confusion_matrix(y_test_fold_tsnerf, y_pred_fold_tsnerf)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_tsnerf = ConfusionMatrixDisplay(confusion_matrix=cm_tsnerf)\n",
    "        disp_tsnerf.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - T-SNE Random Forest (T-SNERF)\")\n",
    "        pname_tsnerf = method + \"_fold_\" + str(fold_number) + \"_TSNE_confusion_matrix.png\"\n",
    "        plt.savefig(pname_tsnerf)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_tsnerf = time.time()\n",
    "    result(all_y_pred_tsnerf, all_y_test_tsnerf, \"T-SNE Random Forest (T-SNERF) with k-fold Cross-Validation\", time_to_train_tsnerf, time_to_predict_fold_tsnerf)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"T-SNE Random Forest (T-SNERF) with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"T-SNE Random Forest\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 60. Projected Gradient Descent (PGD) with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_pgd = []\n",
    "    all_y_pred_pgd = []\n",
    "    start_cv_pgd = time.time()\n",
    "    time_to_predict_fold_pgd = 0\n",
    "    time_to_train_pgd = 0\n",
    "\n",
    "    # Define a simple neural network model\n",
    "    class SimpleNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.fc1 = nn.Linear(X_train.shape[1], 128)\n",
    "            self.fc2 = nn.Linear(128, len(np.unique(y_train)))\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_pgd, test_index_pgd) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_pgd, X_test_fold_pgd = X_train.iloc[train_index_pgd], X_train.iloc[test_index_pgd]\n",
    "        y_train_fold_pgd, y_test_fold_pgd = y_train.iloc[train_index_pgd], y_train.iloc[test_index_pgd]\n",
    "\n",
    "        # Convert pandas DataFrame to PyTorch tensors for this fold\n",
    "        X_train_tensor_pgd = torch.tensor(X_train_fold_pgd.values, dtype=torch.float32)\n",
    "        y_train_tensor_pgd = torch.tensor(y_train_fold_pgd.values, dtype=torch.long)\n",
    "        X_test_tensor_pgd = torch.tensor(X_test_fold_pgd.values, dtype=torch.float32)\n",
    "\n",
    "        # Create a DataLoader for training for this fold\n",
    "        train_dataset_pgd = TensorDataset(X_train_tensor_pgd, y_train_tensor_pgd)\n",
    "        train_loader_pgd = DataLoader(train_dataset_pgd, batch_size=64, shuffle=True)\n",
    "\n",
    "        # Define model, loss function, and optimizer for this fold\n",
    "        model_pgd = SimpleNN()\n",
    "        criterion_pgd = nn.CrossEntropyLoss()\n",
    "        optimizer_pgd = optim.Adam(model_pgd.parameters(), lr=0.001)\n",
    "\n",
    "        # Train the model for this fold\n",
    "        start_train_fold_pgd = time.time()\n",
    "        for epoch in range(10):\n",
    "            for inputs, labels in train_loader_pgd:\n",
    "                optimizer_pgd.zero_grad()\n",
    "                outputs = model_pgd(inputs)\n",
    "                loss = criterion_pgd(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer_pgd.step()\n",
    "        end_train_fold_pgd = time.time()\n",
    "        time_to_train_pgd += end_train_fold_pgd - start_train_fold_pgd\n",
    "\n",
    "        # Evaluate the model for this fold\n",
    "        model_pgd.eval()\n",
    "        start_predict_fold_pgd = time.time()\n",
    "        with torch.no_grad():\n",
    "            y_pred_fold_pgd = model_pgd(X_test_tensor_pgd).argmax(dim=1).detach().numpy()\n",
    "        end_predict_fold_pgd = time.time()\n",
    "        time_to_predict_fold_pgd += end_predict_fold_pgd - start_predict_fold_pgd\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_pgd.extend(y_test_fold_pgd)\n",
    "        all_y_pred_pgd.extend(y_pred_fold_pgd)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_pgd = confusion_matrix(y_test_fold_pgd, y_pred_fold_pgd)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_pgd = ConfusionMatrixDisplay(confusion_matrix=cm_pgd)\n",
    "        disp_pgd.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Projected Gradient Descent (PGD)\")\n",
    "        pname_pgd = method + \"_fold_\" + str(fold_number) + \"_PGD_confusion_matrix.png\"\n",
    "        plt.savefig(pname_pgd)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_pgd = time.time()\n",
    "    result(all_y_pred_pgd, all_y_test_pgd, \"Projected Gradient Descent (PGD)\", time_to_train_pgd, time_to_predict_fold_pgd)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Projected Gradient Descent (PGD) with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Projected Gradient Descent\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 61. Principal Component Analysis (PCA) with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_pca = []\n",
    "    all_y_pred_pca = []\n",
    "    start_cv_pca = time.time()\n",
    "    time_to_predict_fold_pca = 0\n",
    "    time_to_train_pca = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_pca, test_index_pca) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_pca, X_test_fold_pca = X_train.iloc[train_index_pca], X_train.iloc[test_index_pca]\n",
    "        y_train_fold_pca, y_test_fold_pca = y_train.iloc[train_index_pca], y_train.iloc[test_index_pca]\n",
    "\n",
    "        # Convert y_test_fold_pca to integer labels if it's not already\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_test_encoded_fold_pca = label_encoder.fit_transform(y_test_fold_pca)\n",
    "\n",
    "        # Apply PCA to reduce dimensionality for this fold\n",
    "        pca_fold_pca = PCA(n_components=0.95, random_state=42)  # Retain 95% of variance\n",
    "        X_train_fold_pca = pca_fold_pca.fit_transform(X_train_fold_pca)\n",
    "        X_test_fold_pca = pca_fold_pca.transform(X_test_fold_pca)\n",
    "\n",
    "        # Train the classifier on the reduced dimensionality data for this fold\n",
    "        dt_multi_fold_pca = DecisionTreeClassifier(random_state=24)\n",
    "        start_train_fold_pca = time.time()\n",
    "        dt_multi_fold_pca.fit(X_train_fold_pca, y_train_fold_pca)\n",
    "        end_train_fold_pca = time.time()\n",
    "        time_to_train_pca += end_train_fold_pca - start_train_fold_pca\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_pca = time.time()\n",
    "        y_pred_fold_pca = dt_multi_fold_pca.predict(X_test_fold_pca)\n",
    "        end_predict_fold_pca = time.time()\n",
    "        time_to_predict_fold_pca += end_predict_fold_pca - start_predict_fold_pca\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_pca.extend(y_test_encoded_fold_pca)\n",
    "        all_y_pred_pca.extend(y_pred_fold_pca)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_pca = confusion_matrix(y_test_encoded_fold_pca, y_pred_fold_pca)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_pca = ConfusionMatrixDisplay(confusion_matrix=cm_pca)\n",
    "        disp_pca.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - PCA\")\n",
    "        pname_pca = method + \"_fold_\" + str(fold_number) + \"_PCA_confusion_matrix.png\"\n",
    "        plt.savefig(pname_pca)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_pca = time.time()\n",
    "    result(all_y_pred_pca, all_y_test_pca, \"PCA\", time_to_train_pca, time_to_predict_fold_pca)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"PCA with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"PCA\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 48. RBF SVM Algorithm with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_gbkk = []\n",
    "    all_y_pred_gbkk = []\n",
    "    start_cv_gbkk = time.time()\n",
    "    time_to_predict_fold_gbkk = 0\n",
    "    time_to_train_gbkk = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_gbkk, test_index_gbkk) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_gbkk, X_test_fold_gbkk = X_train.iloc[train_index_gbkk], X_train.iloc[test_index_gbkk]\n",
    "        y_train_fold_gbkk, y_test_fold_gbkk = y_train.iloc[train_index_gbkk], y_train.iloc[test_index_gbkk]\n",
    "\n",
    "        # Initialize Gaussian Naive Bayes (GBBK) model for this fold\n",
    "        gbbk_model_fold_gbkk = SVC(kernel='rbf', random_state=24)\n",
    "\n",
    "        # Train the GBBK model for this fold\n",
    "        start_train_fold_gbkk = time.time()\n",
    "        gbbk_model_fold_gbkk.fit(X_train_fold_gbkk, y_train_fold_gbkk)\n",
    "        end_train_fold_gbkk = time.time()\n",
    "        time_to_train_gbkk += end_train_fold_gbkk - start_train_fold_gbkk\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_gbkk = time.time()\n",
    "        y_pred_fold_gbkk = gbbk_model_fold_gbkk.predict(X_test_fold_gbkk)\n",
    "        end_predict_fold_gbkk = time.time()\n",
    "        time_to_predict_fold_gbkk += end_predict_fold_gbkk - start_predict_fold_gbkk\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_gbkk.extend(y_test_fold_gbkk)\n",
    "        all_y_pred_gbkk.extend(y_pred_fold_gbkk)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_gbkk = confusion_matrix(y_test_fold_gbkk, y_pred_fold_gbkk)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_gbkk = ConfusionMatrixDisplay(confusion_matrix=cm_gbkk)\n",
    "        disp_gbkk.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - RBF SVM\")\n",
    "        pname_gbkk = method + \"_fold_\" + str(fold_number) + \"_RBF_confusion_matrix.png\"\n",
    "        plt.savefig(pname_gbkk)\n",
    "        #plt.show()\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_gbkk = time.time()\n",
    "    result(all_y_pred_gbkk, all_y_test_gbkk, \"RBF\", time_to_train_gbkk, time_to_predict_fold_gbkk)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"RBF  with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"RBF\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## Stacked Convolutional Neural Networks with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Convert DataFrame to NumPy array\n",
    "    X = X_train.to_numpy()\n",
    "    y = y_train.to_numpy()\n",
    "\n",
    "    # Reshape the input data for Conv1D layer\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "    # Define the CNN model\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # Adjust the number of units based on the number of classes in your dataset\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Initialize k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits= n_splits_for_cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_cnn = []\n",
    "    all_y_pred_cnn = []\n",
    "    start_cv_cnn = time.time()\n",
    "    time_to_predict_fold_cnn = 0\n",
    "    time_to_train_cnn = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_cnn, test_index_cnn) in enumerate(skf.split(X, y), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_cnn, X_test_fold_cnn = X[train_index_cnn], X[test_index_cnn]\n",
    "        y_train_fold_cnn, y_test_fold_cnn = y[train_index_cnn], y[test_index_cnn]\n",
    "\n",
    "        # Train the model for this fold\n",
    "        start_train_fold_cnn = time.time()\n",
    "        history = model.fit(X_train_fold_cnn, y_train_fold_cnn, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "        end_train_fold_cnn = time.time()\n",
    "        time_to_train_cnn += end_train_fold_cnn - start_train_fold_cnn\n",
    "\n",
    "        # Evaluate the model for this fold\n",
    "        start_predict_fold_cnn = time.time()\n",
    "        y_pred_fold_cnn = model.predict(X_test_fold_cnn)\n",
    "        y_pred_fold_cnn = np.argmax(y_pred_fold_cnn, axis=1)\n",
    "        end_predict_fold_cnn = time.time()\n",
    "        time_to_predict_fold_cnn += end_predict_fold_cnn - start_predict_fold_cnn\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_cnn.extend(y_test_fold_cnn)\n",
    "        all_y_pred_cnn.extend(y_pred_fold_cnn)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_cnn = confusion_matrix(y_test_fold_cnn, y_pred_fold_cnn)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_cnn = ConfusionMatrixDisplay(confusion_matrix=cm_cnn)\n",
    "        disp_cnn.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Stacked Convolutional Neural Networks\")\n",
    "        pname_cnn = method + \"_fold_\" + str(fold_number) + \"_Stacked_Convolutional_Neural_Networks_confusion_matrix.png\"\n",
    "        plt.savefig(pname_cnn)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_cnn = time.time()\n",
    "    result(all_y_pred_cnn, all_y_test_cnn, \"Stacked Convolutional Neural Networks with k-fold Cross-Validation\", time_to_train_cnn, time_to_predict_fold_cnn)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Stacked Convolutional Neural Networks with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Stacked Convolutional Neural Networks\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 64. Simulated Annealing (SA) with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Define the objective function for optimization\n",
    "\n",
    "    # Convert X_train and y_train to NumPy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # Convert X_test to NumPy array if it's not already\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    # Reshape the input data if needed\n",
    "    if X_train.ndim > 2:\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    if X_test.ndim > 2:\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    def objective_function(params):\n",
    "        max_depth, min_samples_split, min_samples_leaf = params\n",
    "\n",
    "        # Create and train the Decision Tree model with given hyperparameters\n",
    "        dt_model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=24)\n",
    "        dt_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict using the trained model\n",
    "        y_pred = dt_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model - Example: using accuracy\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "        # Return the negative of the accuracy (since we want to minimize)\n",
    "        return -accuracy\n",
    "    # Simulated Annealing hyperparameter optimization\n",
    "    def simulated_annealing(objective_function, space, n_calls=50, initial_temperature=100.0, cooling_rate=0.95):\n",
    "        best_params = None\n",
    "        best_score = float('-inf')\n",
    "        temperature = initial_temperature\n",
    "\n",
    "        current_params = [np.random.randint(low, high + 1) for low, high in space]\n",
    "\n",
    "        for _ in range(n_calls):\n",
    "            next_params = [np.random.randint(low, high + 1) for low, high in space]\n",
    "\n",
    "            current_score = objective_function(current_params)\n",
    "            next_score = objective_function(next_params)\n",
    "\n",
    "            if next_score > current_score or np.random.rand() < np.exp((next_score - current_score) / temperature):\n",
    "                current_params = next_params\n",
    "                current_score = next_score\n",
    "\n",
    "            if current_score > best_score:\n",
    "                best_params = current_params\n",
    "                best_score = current_score\n",
    "\n",
    "            temperature *= cooling_rate\n",
    "\n",
    "        return best_params\n",
    "    # Define the search space for hyperparameters\n",
    "    space = [(1, 20),  # max_depth\n",
    "            (2, 20),  # min_samples_split\n",
    "            (1, 20)]  # min_samples_leaf\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_sa = []\n",
    "    all_y_pred_sa = []\n",
    "    start_cv_sa = time.time()\n",
    "    time_to_predict_fold_sa = 0\n",
    "    time_to_train_sa = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_sa, test_index_sa) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_sa, X_test_fold_sa = X_train[train_index_sa], X_train[test_index_sa]\n",
    "        y_train_fold_sa, y_test_fold_sa = y_train[train_index_sa], y_train[test_index_sa]\n",
    "\n",
    "        # Run the optimization using Simulated Annealing for this fold\n",
    "        start_train_fold_sa = time.time()\n",
    "        best_params = simulated_annealing(objective_function, space)\n",
    "        end_train_fold_sa = time.time()\n",
    "\n",
    "        # Train the final model with the best hyperparameters for this fold\n",
    "        dt_model_final_fold_sa = DecisionTreeClassifier(max_depth=best_params[0], min_samples_split=best_params[1], min_samples_leaf=best_params[2], random_state=24)\n",
    "        dt_model_final_fold_sa.fit(X_train_fold_sa, y_train_fold_sa)\n",
    "\n",
    "        # Predict using the final model for this fold\n",
    "        start_predict_fold_sa = time.time()\n",
    "        y_pred_fold_sa = dt_model_final_fold_sa.predict(X_test_fold_sa)\n",
    "        end_predict_fold_sa = time.time()\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_sa.extend(y_test_fold_sa)\n",
    "        all_y_pred_sa.extend(y_pred_fold_sa)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_sa = confusion_matrix(y_test_fold_sa, y_pred_fold_sa)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_sa = ConfusionMatrixDisplay(confusion_matrix=cm_sa)\n",
    "        disp_sa.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - SA\")\n",
    "        pname_sa = method + \"_fold_\" + str(fold_number) + \"_SA_confusion_matrix.png\"\n",
    "        plt.savefig(pname_sa)\n",
    "        #plt.show()\n",
    "\n",
    "        # Calculate time metrics\n",
    "        time_to_train_fold_sa = end_train_fold_sa - start_train_fold_sa\n",
    "        time_to_predict_fold_sa = end_predict_fold_sa - start_predict_fold_sa\n",
    "\n",
    "        # Update overall time metrics\n",
    "        time_to_train_sa += time_to_train_fold_sa\n",
    "        time_to_predict_fold_sa += time_to_predict_fold_sa\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_sa = time.time()\n",
    "    result(all_y_pred_sa, all_y_test_sa, \"Simulated Annealing (SA)\", time_to_train_sa, time_to_predict_fold_sa)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"SA with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Simulated Annealing\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## SVM with Optimization and k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Reshape input data to have two dimensions\n",
    "    X_flattened = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    # Initialize k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits= n_splits_for_cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_svm = []\n",
    "    all_y_pred_svm = []\n",
    "    start_cv_svm = time.time()\n",
    "    time_to_predict_fold_svm = 0\n",
    "    time_to_train_svm = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_svm, test_index_svm) in enumerate(skf.split(X_flattened, y), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_svm, X_test_fold_svm = X_flattened[train_index_svm], X_flattened[test_index_svm]\n",
    "        y_train_fold_svm, y_test_fold_svm = y[train_index_svm], y[test_index_svm]\n",
    "\n",
    "        # Create and train the SVM model with optimization for this fold\n",
    "        start_train_fold_svm = time.time()\n",
    "        svm_model_fold = SVC(kernel='linear', C=1.0)  # Example hyperparameters, adjust as needed\n",
    "        svm_model_fold.fit(X_train_fold_svm, y_train_fold_svm)\n",
    "        end_train_fold_svm = time.time()\n",
    "        time_to_train_svm += end_train_fold_svm - start_train_fold_svm\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_svm = time.time()\n",
    "        y_pred_fold_svm = svm_model_fold.predict(X_test_fold_svm)\n",
    "        end_predict_fold_svm = time.time()\n",
    "        time_to_predict_fold_svm += end_predict_fold_svm - start_predict_fold_svm\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_svm.extend(y_test_fold_svm)\n",
    "        all_y_pred_svm.extend(y_pred_fold_svm)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_svm = confusion_matrix(y_test_fold_svm, y_pred_fold_svm)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm)\n",
    "        disp_svm.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - SVM with Optimization\")\n",
    "        pname_svm = method + \"_fold_\" + str(fold_number) + \"_SVM_with_Optimization_confusion_matrix.png\"\n",
    "        plt.savefig(pname_svm)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_svm = time.time()\n",
    "    result(all_y_pred_svm, all_y_test_svm, \"SVM with Optimization and k-fold Cross-Validation\", time_to_train_svm, time_to_predict_fold_svm)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"SVM with Optimization and k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"SVM\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 66 Stacking Classifier with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Define base classifiers\n",
    "    base_classifiers = [\n",
    "        ('dt', DecisionTreeClassifier(random_state=24)),\n",
    "        ('rf', RandomForestClassifier(random_state=24)),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ]\n",
    "\n",
    "    # Initialize Stacking Classifier with base classifiers\n",
    "    stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=DecisionTreeClassifier())\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_stacking = []\n",
    "    all_y_pred_stacking = []\n",
    "    start_cv_stacking = time.time()\n",
    "    time_to_predict_fold_stacking = 0\n",
    "    time_to_train_stacking = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_stacking, test_index_stacking) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_stacking, X_test_fold_stacking = X_train[train_index_stacking], X_train[test_index_stacking]\n",
    "        y_train_fold_stacking, y_test_fold_stacking = y_train[train_index_stacking], y_train[test_index_stacking]\n",
    "\n",
    "        # Train the Stacking Classifier for this fold\n",
    "        start_train_fold_stacking = time.time()\n",
    "        stacking_classifier.fit(X_train_fold_stacking, y_train_fold_stacking)\n",
    "        end_train_fold_stacking = time.time()\n",
    "        time_to_train_stacking += end_train_fold_stacking - start_train_fold_stacking\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_stacking = time.time()\n",
    "        y_pred_fold_stacking = stacking_classifier.predict(X_test_fold_stacking)\n",
    "        end_predict_fold_stacking = time.time()\n",
    "        time_to_predict_fold_stacking += end_predict_fold_stacking - start_predict_fold_stacking\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_stacking.extend(y_test_fold_stacking)\n",
    "        all_y_pred_stacking.extend(y_pred_fold_stacking)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_stacking = confusion_matrix(y_test_fold_stacking, y_pred_fold_stacking)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_stacking = ConfusionMatrixDisplay(confusion_matrix=cm_stacking)\n",
    "        disp_stacking.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Stacking Classifier\")\n",
    "        pname_stacking = method + \"_fold_\" + str(fold_number) + \"_Stacking_Classifier_confusion_matrix.png\"\n",
    "        plt.savefig(pname_stacking)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    end_cv_stacking = time.time()\n",
    "\n",
    "\n",
    "    result(all_y_pred_stacking, all_y_test_stacking, \"Stacking Classifier\",  time_to_train_stacking, time_to_predict_fold_stacking)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Stacking Classifier with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Stacking Classifier\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 67 Stacking Dilated Convolutional Autoencoders with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # You may replace this with your DCAE model\n",
    "    base_model = make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=24))\n",
    "\n",
    "    # Define the stacking classifier\n",
    "    stacking_model = StackingClassifier(estimators=[('dt', base_model)], final_estimator=DecisionTreeClassifier())\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_stacking_dcae = []\n",
    "    all_y_pred_stacking_dcae = []\n",
    "    time_to_predict_fold_stacking_dcae = 0\n",
    "    time_to_train_stacking_dcae = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_stacking_dcae, test_index_stacking_dcae) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_stacking_dcae, X_test_fold_stacking_dcae = X_train[train_index_stacking_dcae], X_train[test_index_stacking_dcae]\n",
    "        y_train_fold_stacking_dcae, y_test_fold_stacking_dcae = y_train[train_index_stacking_dcae], y_train[test_index_stacking_dcae]\n",
    "\n",
    "        # Training the stacking model for this fold\n",
    "        start_train_fold_stacking_dcae = time.time()\n",
    "        stacking_model.fit(X_train_fold_stacking_dcae, y_train_fold_stacking_dcae)\n",
    "        end_train_fold_stacking_dcae = time.time()\n",
    "        time_to_train_stacking_dcae += end_train_fold_stacking_dcae - start_train_fold_stacking_dcae\n",
    "\n",
    "        # Predict using the trained stacking model for this fold\n",
    "        start_predict_fold_stacking_dcae = time.time()\n",
    "        y_pred_fold_stacking_dcae = stacking_model.predict(X_test_fold_stacking_dcae)\n",
    "        end_predict_fold_stacking_dcae = time.time()\n",
    "        time_to_predict_fold_stacking_dcae += end_predict_fold_stacking_dcae - start_predict_fold_stacking_dcae\n",
    "\n",
    "        # Generate confusion matrix for this fold\n",
    "        cm_stacking_dcae = confusion_matrix(y_test_fold_stacking_dcae, y_pred_fold_stacking_dcae)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_stacking_dcae = ConfusionMatrixDisplay(confusion_matrix=cm_stacking_dcae)\n",
    "        disp_stacking_dcae.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Stacking DCAE\")\n",
    "        pname_stacking_dcae = method + \"_fold_\" + str(fold_number) + \"_Stacking_DCAE_confusion_matrix.png\"\n",
    "        plt.savefig(pname_stacking_dcae)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_stacking_dcae.extend(y_test_fold_stacking_dcae)\n",
    "        all_y_pred_stacking_dcae.extend(y_pred_fold_stacking_dcae)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    result(all_y_pred_stacking_dcae, all_y_test_stacking_dcae, \"Stacking Dilated Convolutional Autoencoders\", time_to_train_stacking_dcae, time_to_predict_fold_stacking_dcae)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Stacking Dilated Convolutional Autoencoders with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Stacking Dilated Convolutional\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 68 Temporal Deep Feedforward Neural Network with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Define the structure of the Temporal DNN model\n",
    "    y_train = pd.Series(y_train)\n",
    "    num_classes = y_train.nunique()\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')  # Assuming len(y_train.unique()) is the number of classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_temporal_dnn = []\n",
    "    all_y_pred_temporal_dnn = []\n",
    "    time_to_predict_fold_temporal_dnn = 0\n",
    "    time_to_train_temporal_dnn = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_temporal_dnn, test_index_temporal_dnn) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_temporal_dnn, X_test_fold_temporal_dnn = X_train[train_index_temporal_dnn], X_train[test_index_temporal_dnn]\n",
    "        y_train_fold_temporal_dnn, y_test_fold_temporal_dnn = y_train[train_index_temporal_dnn], y_train[test_index_temporal_dnn]\n",
    "\n",
    "        # Define the Temporal DNN model for this fold\n",
    "        model_fold_temporal_dnn = Sequential([\n",
    "            Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            Dropout(0.5),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model_fold_temporal_dnn.compile(optimizer=Adam(),\n",
    "                                        loss='sparse_categorical_crossentropy',\n",
    "                                        metrics=['accuracy'])\n",
    "\n",
    "        # Training the Temporal DNN model for this fold\n",
    "        start_train_fold_temporal_dnn = time.time()\n",
    "        history = model_fold_temporal_dnn.fit(X_train_fold_temporal_dnn, y_train_fold_temporal_dnn, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "        end_train_fold_temporal_dnn = time.time()\n",
    "        time_to_train_temporal_dnn += end_train_fold_temporal_dnn - start_train_fold_temporal_dnn\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_temporal_dnn = time.time()\n",
    "        y_pred_probs_fold_temporal_dnn = model_fold_temporal_dnn.predict(X_test_fold_temporal_dnn)\n",
    "        y_pred_fold_temporal_dnn = np.argmax(y_pred_probs_fold_temporal_dnn, axis=1)\n",
    "        end_predict_fold_temporal_dnn = time.time()\n",
    "        time_to_predict_fold_temporal_dnn += end_predict_fold_temporal_dnn - start_predict_fold_temporal_dnn\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_temporal_dnn.extend(y_test_fold_temporal_dnn)\n",
    "        all_y_pred_temporal_dnn.extend(y_pred_fold_temporal_dnn)\n",
    "\n",
    "        # Generate confusion matrix for this fold\n",
    "        cm_temporal_dnn = confusion_matrix(y_test_fold_temporal_dnn, y_pred_fold_temporal_dnn)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_temporal_dnn = ConfusionMatrixDisplay(confusion_matrix=cm_temporal_dnn)\n",
    "        disp_temporal_dnn.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Temporal DNN\")\n",
    "        pname_temporal_dnn = method + \"_fold_\" + str(fold_number) + \"_Temporal_DNN_confusion_matrix.png\"\n",
    "        plt.savefig(pname_temporal_dnn)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    result(all_y_pred_temporal_dnn, all_y_test_temporal_dnn, \"Temporal Deep Feedforward Neural Network\", time_to_train_temporal_dnn, time_to_predict_fold_temporal_dnn)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Temporal Deep Feedforward Neural Network with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Temporal Deep Feedforward Neural\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 69 Voting Classifier with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize individual classifiers\n",
    "    dt1 = DecisionTreeClassifier(random_state=24)\n",
    "    dt2 = DecisionTreeClassifier(random_state=42)\n",
    "    # Add more classifiers if needed\n",
    "\n",
    "    # Create the Voting Classifier\n",
    "    voting_clf = VotingClassifier(estimators=[\n",
    "        ('dt1', dt1),\n",
    "        ('dt2', dt2),\n",
    "        # Add more classifiers here if needed\n",
    "    ], voting='hard')  # You can use 'soft' voting if your classifiers support probability prediction\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_voting = []\n",
    "    all_y_pred_voting = []\n",
    "    time_to_predict_fold_voting = 0\n",
    "    time_to_train_voting = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_voting, test_index_voting) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_voting, X_test_fold_voting = X_train[train_index_voting], X_train[test_index_voting]\n",
    "        y_train_fold_voting, y_test_fold_voting = y_train[train_index_voting], y_train[test_index_voting]\n",
    "\n",
    "        # Train the Voting Classifier for this fold\n",
    "        start_train_fold_voting = time.time()\n",
    "        voting_clf.fit(X_train_fold_voting, y_train_fold_voting)\n",
    "        end_train_fold_voting = time.time()\n",
    "        time_to_train_voting += end_train_fold_voting - start_train_fold_voting\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_voting = time.time()\n",
    "        y_pred_fold_voting = voting_clf.predict(X_test_fold_voting)\n",
    "        end_predict_fold_voting = time.time()\n",
    "        time_to_predict_fold_voting += end_predict_fold_voting - start_predict_fold_voting\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_voting.extend(y_test_fold_voting)\n",
    "        all_y_pred_voting.extend(y_pred_fold_voting)\n",
    "\n",
    "        # Generate confusion matrix for this fold\n",
    "        cm_voting = confusion_matrix(y_test_fold_voting, y_pred_fold_voting)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_voting = ConfusionMatrixDisplay(confusion_matrix=cm_voting)\n",
    "        disp_voting.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - Voting Classifier\")\n",
    "        pname_voting = method + \"_fold_\" + str(fold_number) + \"_Voting_Classifier_confusion_matrix.png\"\n",
    "        plt.savefig(pname_voting)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    result(all_y_pred_voting, all_y_test_voting, \"Voting Classifier\", time_to_train_voting, time_to_predict_fold_voting)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"Voting Classifier with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"Voting Classifier\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 70 GBT with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    # Initialize the GBT classifier\n",
    "    gbt_model = GradientBoostingClassifier(random_state=24)\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_gbt = []\n",
    "    all_y_pred_gbt = []\n",
    "    time_to_predict_fold_gbt = 0\n",
    "    time_to_train_gbt = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_gbt, test_index_gbt) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_gbt, X_test_fold_gbt = X_train[train_index_gbt], X_train[test_index_gbt]\n",
    "        y_train_fold_gbt, y_test_fold_gbt = y_train[train_index_gbt], y_train[test_index_gbt]\n",
    "\n",
    "        # Train the GBT model for this fold\n",
    "        start_train_fold_gbt = time.time()\n",
    "        gbt_model.fit(X_train_fold_gbt, y_train_fold_gbt)\n",
    "        end_train_fold_gbt = time.time()\n",
    "        time_to_train_gbt += end_train_fold_gbt - start_train_fold_gbt\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_gbt = time.time()\n",
    "        y_pred_fold_gbt = gbt_model.predict(X_test_fold_gbt)\n",
    "        end_predict_fold_gbt = time.time()\n",
    "        time_to_predict_fold_gbt += end_predict_fold_gbt - start_predict_fold_gbt\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_gbt.extend(y_test_fold_gbt)\n",
    "        all_y_pred_gbt.extend(y_pred_fold_gbt)\n",
    "\n",
    "        # Generate confusion matrix for this fold\n",
    "        cm_gbt = confusion_matrix(y_test_fold_gbt, y_pred_fold_gbt)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_gbt = ConfusionMatrixDisplay(confusion_matrix=cm_gbt)\n",
    "        disp_gbt.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - GBT\")\n",
    "        pname_gbt = method + \"_fold_\" + str(fold_number) + \"_GBT_confusion_matrix.png\"\n",
    "        plt.savefig(pname_gbt)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    result(all_y_pred_gbt, all_y_test_gbt, \"GBT\", time_to_train_gbt, time_to_predict_fold_gbt)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"GBT with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"GBT\")\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "try:\n",
    "    \"\"\"## 71 CART with k-fold Cross-Validation\"\"\"\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "            self.feature_index = feature_index  # Index of feature to split on\n",
    "            self.threshold = threshold  # Threshold value to split on\n",
    "            self.left = left  # Left subtree\n",
    "            self.right = right  # Right subtree\n",
    "            self.value = value  # Class label (for leaf nodes)\n",
    "\n",
    "    class CART:\n",
    "        def __init__(self, max_depth=None):\n",
    "            self.max_depth = max_depth\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            self.n_classes = len(set(y))\n",
    "            self.n_features = X.shape[1]\n",
    "            self.tree = self._build_tree(X, y)\n",
    "\n",
    "        def _build_tree(self, X, y, depth=0):\n",
    "            n_samples, n_features = X.shape\n",
    "            n_labels = len(set(y))\n",
    "\n",
    "            # Stop conditions\n",
    "            if depth == self.max_depth or n_labels == 1:\n",
    "                value = max(set(y), key=list(y).count)\n",
    "                return Node(value=value)\n",
    "\n",
    "            # Find best split\n",
    "            best_gini = float('inf')\n",
    "            best_feature_index = None\n",
    "            best_threshold = None\n",
    "\n",
    "            for feature_index in range(n_features):\n",
    "                thresholds = sorted(set(X[:, feature_index]))\n",
    "                for threshold in thresholds:\n",
    "                    left_indices = (X[:, feature_index] <= threshold)\n",
    "                    right_indices = (X[:, feature_index] > threshold)\n",
    "\n",
    "                    left_gini = self._gini(y[left_indices])\n",
    "                    right_gini = self._gini(y[right_indices])\n",
    "\n",
    "                    gini = (len(left_indices) * left_gini + len(right_indices) * right_gini) / n_samples\n",
    "\n",
    "                    if gini < best_gini:\n",
    "                        best_gini = gini\n",
    "                        best_feature_index = feature_index\n",
    "                        best_threshold = threshold\n",
    "\n",
    "            # Split data\n",
    "            left_indices = (X[:, best_feature_index] <= best_threshold)\n",
    "            right_indices = (X[:, best_feature_index] > best_threshold)\n",
    "\n",
    "            left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "            right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "            return Node(best_feature_index, best_threshold, left_subtree, right_subtree)\n",
    "\n",
    "        def _gini(self, y):\n",
    "            n_samples = len(y)\n",
    "            gini = 1.0\n",
    "            for label in set(y):\n",
    "                proportion = (y == label).sum() / n_samples\n",
    "                gini -= proportion ** 2\n",
    "            return gini\n",
    "\n",
    "        def predict(self, X):\n",
    "            return [self._predict_one(sample, self.tree) for sample in X]\n",
    "\n",
    "        def _predict_one(self, sample, node):\n",
    "            if node.value is not None:\n",
    "                return node.value\n",
    "\n",
    "            if sample[node.feature_index] <= node.threshold:\n",
    "                return self._predict_one(sample, node.left)\n",
    "            else:\n",
    "                return self._predict_one(sample, node.right)\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    all_y_test_cart = []\n",
    "    all_y_pred_cart = []\n",
    "    time_to_predict_fold_cart = 0\n",
    "    time_to_train_cart = 0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold_number, (train_index_cart, test_index_cart) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        # Split data into train and test sets for the fold\n",
    "        X_train_fold_cart, X_test_fold_cart = X_train[train_index_cart], X_train[test_index_cart]\n",
    "        y_train_fold_cart, y_test_fold_cart = y_train[train_index_cart], y_train[test_index_cart]\n",
    "\n",
    "        # Create and train the CART model for this fold\n",
    "        cart_model_fold = CART(max_depth=1)\n",
    "        start_train_fold_cart = time.time()\n",
    "        cart_model_fold.fit(X_train_fold_cart, y_train_fold_cart)\n",
    "        end_train_fold_cart = time.time()\n",
    "        time_to_train_cart += end_train_fold_cart - start_train_fold_cart\n",
    "\n",
    "        # Predict using the trained model for this fold\n",
    "        start_predict_fold_cart = time.time()\n",
    "        y_pred_fold_cart = cart_model_fold.predict(X_test_fold_cart)\n",
    "        end_predict_fold_cart = time.time()\n",
    "        time_to_predict_fold_cart += end_predict_fold_cart - start_predict_fold_cart\n",
    "\n",
    "        # Append metrics to lists\n",
    "        all_y_test_cart.extend(y_test_fold_cart)\n",
    "        all_y_pred_cart.extend(y_pred_fold_cart)\n",
    "\n",
    "        # Generate confusion matrix and display for the fold\n",
    "        cm_cart = confusion_matrix(y_test_fold_cart, y_pred_fold_cart)\n",
    "        plt.rcParams['figure.figsize'] = 8, 8\n",
    "        sns.set_style(\"white\")\n",
    "        disp_cart = ConfusionMatrixDisplay(confusion_matrix=cm_cart)\n",
    "        disp_cart.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Fold {fold_number} Confusion Matrix - CART\")\n",
    "        pname_cart = method + \"_fold_\" + str(fold_number) + \"_CART_confusion_matrix.png\"\n",
    "        plt.savefig(pname_cart)\n",
    "\n",
    "    # Calculate overall performance metrics\n",
    "    result(all_y_pred_cart, all_y_test_cart, \"CART\", time_to_train_cart, time_to_predict_fold_cart)\n",
    "\n",
    "    outfile.close()\n",
    "    outfile = open(fname, 'a')\n",
    "    print(\"CART with k-fold Cross-Validation Completed :)  \")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(str(e))\n",
    "    separator(\"CART\")\n",
    "\n",
    "## **Closing the outfile**\n",
    "outfile.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Set the path to your directory\n",
    "directory_path = '.'\n",
    "\n",
    "# Patterns to look for in filenames\n",
    "patterns = ['Metrics_fold_1', 'Metrics_fold_2', 'Metrics_fold_3', 'Metrics_fold_4']\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    # Check if the file is a PNG and contains any of the patterns\n",
    "    if filename.endswith('.png') and any(pattern in filename for pattern in patterns):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            # Delete the file\n",
    "            os.remove(file_path)\n",
    "            print(f'Deleted: {file_path}')\n",
    "        except Exception as e:\n",
    "            print(f'Error deleting {file_path}: {e}')\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "# Your code goes here...\n",
    "\n",
    "# Get the ending time\n",
    "end_time_1 = datetime.datetime.now()\n",
    "\n",
    "# Calculate the total time required for execution\n",
    "total_time = end_time_1 - start_time_1\n",
    "\n",
    "# Format the start time, end time, and total time\n",
    "start_time_str = start_time_1.strftime(\"%d-%m-%Y %H:%M\")\n",
    "end_time_str = end_time_1.strftime(\"%d-%m-%Y %H:%M\")\n",
    "total_time_str = str(total_time)\n",
    "\n",
    "# Write the start time, end time, and total time to the file\n",
    "with open(\"time.txt\", \"a\") as file:\n",
    "    file.write(f\"Start time: {start_time_str}\\n\")\n",
    "    file.write(f\"End time: {end_time_str}\\n\")\n",
    "    file.write(f\"Total time: {total_time_str}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b58aa-5138-401c-9e5b-89b56777d02b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
